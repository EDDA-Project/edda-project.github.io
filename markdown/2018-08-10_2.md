*Preliminary Results*

[Gaussian Mixture Modeling (GMM)](#Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study)
of large-scale cosmological data sets can take a long time, due to that typical
GMM methods will attempt to train the data in one go. This requires saving all
of the intermediate data from simulations, and then constructing (training) the
GMM model. This is inefficient, especially for petascale and exascale
supercomputers, as the GMM modeling requires massive amounts of memory and
storage. To reduce the I/O burden of constructing the GMM model in the first
place, we are investigating building the GMM model through *incremental
training.*

Incremental GMM training builds the data model by sampling an existing GMM
model, adding new data, and then retraining on that data set. An example workflow
would be:

1. Run [Nyx](https://github.com/AMReX-Astro/Nyx) for several input parameters
2. Train a GMM model on Nyx's output data
3. Run Nyx for several more input parameters
4. Sample the GMM model and add the new data from step (3)
5. Train a new GMM model on the data from step (4)
6. Go to step (3)

This amortizes the cost of building a statistical GMM model for simulations,
by spreading the I/O cost and GMM building cost through incremental steps. In our
initial studies, we achieve approximately 0.64% RSME error on reconstruction
when allowing the number of Gaussian parameters to grow. If we keep the number
of Gaussians to a fixed number, such as 5 Gaussians, the errors are larger,
but within a tolerable range of 1% RSME.

In future work, we will investigate the application of *in situ* GMM training,
as our incremental method would allow for the integration of GMM methods
directly into simulations. This would bypass the need for post-processing GMM
training, reducing the storage needs, and building the GMM model as the
simulation runs, allowing for faster scientific analysis turn-around times.

#### Incremental GMM reconstruction (x-axis incremental simulation run, y-axis RMSE)

![Incremental training](images/2018-08-10/Picture11.png)
![Incremental training](images/2018-08-10/Picture12.png)
![Incremental training](images/2018-08-10/Picture13.png)

*Jiayi Xu, Zarija Lukic, and Jon Woodring*

[Link to poster with original resolution images](files/2018-08-10/OneSlideSR.pptx)
