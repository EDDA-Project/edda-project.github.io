![Overview of the system](images/2018-4-5/wang_1.png)

![Isabel -- left original, right proxy](images/2018-4-5/wang_2.png)
![Turbine -- left original, right proxy](images/2018-4-5/wang_3.png)

Analyzing scientific datasets created from simulations on modern supercomputers
is a daunting challenge due to the fast pace at which these datasets continue
to grow. Low cost post analysis machines used by scientists to view and analyze
these massive datasets are severely limited by their deficiencies in storage
bandwidth, capacity, and computational power. Trying to simply move these
datasets to these platforms is infeasible. Any approach to view and analyze
these datasets on post analysis machines will have to effectively address the
inevitable problem of data loss. Image based approaches are well suited for
handling very large datasets on low cost platforms.  Three challenges with
these approaches are how to effectively represent the original data with
minimal data loss, analyze the data in regards to transfer function
exploration, which is a key analysis tool, and quantify the error from data
loss during analysis. We present a novel image based approach using
distributions to preserve data integrity. At each view sample, view dependent
data is summarized at each pixel with distributions to define a compact proxy
for the original dataset. We present this representation along with how to
manipulate and render large scale datasets on post analysis machines. We show
that our approach is a good trade off between rendering quality and interactive
speed and provides uncertainty quantification for the information that is lost.

