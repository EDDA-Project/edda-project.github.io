<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>EDDA: Extreme-scale Distribution-based Data Analysis</title><link rel="stylesheet" href="stylesheets/styles.css"><link rel="stylesheet" href="stylesheets/pygment_trac.css"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><!--if lt IE 9script(src='//html5shiv.googlecode.com/svn/trunk/html5.js')--></head><body><a href="#Logo-Explanation"><img src="images/edda.png" width="25%"></a><h1> <a href="https://github.com/EDDA-Project">EDDA: Extreme-scale Distribution-based Data Analysis</a></h1><h3>The Ohio State University, Argonne National Laboratory,
Los Alamos National Laboratory<br>Han-Wei Shen (OSU), Tom Peterka (ANL), Jon Woodring (LANL)<br>Gagan Agrawal (OSU), Huamin Wang (OSU), Joanne Wendelberger (LANL)</h3><h3><a href="http://science.energy.gov/ascr/">DOE SC ASCR </a>project on the <a href="#Introduction">research for using distributions
as a proxy for large-scale data analysis.</a></h3><small>LA-UR-15-20108</small><hr><p><h3>Towards Statistically-based Error Metrics for Computationally-driven Data Triage</h3><a name="Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/size-time-tradeoff.png" alt="size vs. accuracy"></p>
<p>Computing power has increased faster than the amount of storage bandwidth or the ability to read stored data. Thus, researchers can perform large-scale, high-resolution simulations, but they are unable to load all the data into local memory. Performing queries becomes necessary; however, as the size of the data increases, queries will also be time-consuming.  Bitmap indexing was originally developed for faster query processing of read-mostly data, but has recently been used for scientific data management. It consists of a set of bit vectors, where one vector corresponds to a distinct attribute value or range of values. Each bit is mapped to a record, and the bit value is 1 if the record matches the property in focus.  Bitmap indexing is able to perform complex logical operations quickly, but for floating-point attributes, the bitmap indexing will be lossy because the bit vector values must be binned.  Different strategies have been proposed to bin bit vectors value.  </p>
<p>A review of the current literature on bitmap indexing has identified the need for a comparative error metric to assess the binning scheme of bit vectors.  This metric will incorporate multiple, conflicting criteria, such as query processing speed and index size, provide uncertainty on the assessment, and allow for a sensitivity analysis. This error metric will eventually be used as the bitmap index is being created in situ to indicate when the current binning strategy is no longer optimal.</p>
<p>Emily Casleton (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>), Jon Woodring (<em>LANL</em>)</p>
</p><a href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage"><small> 
Link to this post</small></a><hr><br><h3>A Generalized Framework for Comparing across Data Representations</h3><a name="A-Generalized-Framework-for-Comparing-across-Data-Representations" href="#A-Generalized-Framework-for-Comparing-across-Data-Representations" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/scoreboard.png" alt="scoreboard"></p>
<p>In the era of big data analytics, efficient data transformation and summarization is becoming a popular approach for big data handling. Since it is almost impossible to store all the raw data, an information rich data representation can benefit data analysts immensely by reducing the size of the data to a manageable scale yet preserving the features in the data with high accuracy. In order to perform this task, we need to identify data summarization techniques which are compact, easy to compute and represent. Furthermore, a comparative framework is also need to be devised which will take different data summarization techniques and compare their effectiveness in terms of several critical parameters such as representation accuracy, storage cost, computation time, errors incurred etc. This work aims at producing such a comparative framework. Majority of the data summarization techniques try to prioritize the data by partitioning it into smaller regions and summarizing each region with appropriate representatives. Each of such representation algorithms in general can be divided into three steps: (1) data partitioning, (2) partition summarization and (3) error estimation. Therefore the accuracy and effectiveness of those representative algorithms is of prime importance to data scientists. To address these requirements, we have created a score-boarding framework which accepts several data summarization techniques and performs a global scale parameter study on them. Such a parameter study scheme is able to identify data specific transformation and summarization techniques which yields the best representation for the data. Since these transformation and summarization methods can not capture all the information the data has, appropriate error metrics are also an importance parameter in this study. By performing a comprehensive parameter study on a data set, we can analyze the results and find the best scheme of data summarization and the associated combination of parameters for it. The output of our framework is a database table where we keep track of all the test combinations and the results obtained by them in terms of data reduction, effectiveness of the summarization scheme, time taken to compute etc. By querying the output database, we can easily find the most effective algorithm and the associated parameter combinations which obtained the best result. </p>
<p>Soumya Dutta (<em>LANL</em>, <em>OSU</em>), Emily Casleton (<em>LANL</em>), Ayan Biswas (<em>LANL</em>, <em>OSU</em>), Jon Woodring (<em>LANL</em>), Jim Ahrens (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>)</p>
</p><a href="#A-Generalized-Framework-for-Comparing-across-Data-Representations"><small> 
Link to this post</small></a><hr><br><h3>Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis</h3><a name="Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/gmm-tracking.png" alt="GMM-based tracking"></p>
<p>Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7192664">Link to paper</a></p>
</p><a href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis"><small> 
Link to this post</small></a><hr><br><h3>Efficient Local Histogram Searching via Bitmap Indexing</h3><a name="Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-searching.png" alt="local histogram search"></p>
<p>Representing features by local histograms is a proven technique in several volume analysis and visualization applications including feature tracking and transfer function design. The efficiency of these applications, however, is hampered by the high computational complexity of local histogram computation and matching. In this paper, we propose a novel algorithm to accelerate local histogram search by leveraging bitmap indexing. Our method avoids exhaustive searching of all voxels in the spatial domain by examining only the voxels whose values fall within the value range of user-defined local features and their neighborhood. Based on the idea that the value range of local features is in general much smaller than the dynamic range of the entire dataset, we propose a local voting scheme to construct the local histograms so that only a small number of voxels need to be examined. Experimental results show that our method can reduce much computational workload compared to the conventional approaches. To demonstrate the utility of our method, an interactive interface was developed to assist users in defining target features as local histograms and identify the locations of these features in the dataset.</p>
<p><a href="http://onlinelibrary.wiley.com/doi/10.1111/cgf.12620/abstract">Link to paper</a></p>
</p><a href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing"><small> 
Link to this post</small></a><hr><br><h3>A Compact Multivariate Histogram Representation for Query-driven Visualization</h3><a name="A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-query.png" alt="pipeline for histogram query"></p>
<p>As the size of data continues to increase, distribution-based methods become increasingly more important for data summarization and queries. To represent the distribution from a dataset without relying on a particular parametric model, histograms are widely used in many applications as it is simple to create and efficient to query. For multivariate scientific datasets, however, storing multivariate histograms in the form of multi-dimensional arrays is very expensive as the size of the histogram grows exponentially with the number of variables. In this paper, we present a compact structure to store multivariate histograms to reduce its huge space cost while supporting different kinds of histogram queries efficiently. A data space transformation is employed first to transform the large multi-dimensional array to a much smaller array. Dictionaries are constructed to encode this transformation. Then, the multivariate histogram is represented as a sequence of index and frequency pairs where the indices are represented as bitstrings computed from a space filling curve traversal of the transformed array. With this compact representation, the storage cost for the histograms is reduced. Based on our representation, we also present several common types of queries such as histogram marginalization, bin-merging and computation of conditional probability. We parallelize both the histogram computation and queries to improve its efficiency. We present several query-driven visualization applications to explore and analyze multivariate scientific datasets. Experimental results to study the performance of our framework in terms of scalability and space cost are also discussed.</p>
<p>Kewei Lu, Han-Wei Shen. <em>The 5th IEEE Symposium on Large Data Analysis and Visualization</em>, 2015.</p>
</p><a href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization"><small> 
Link to this post</small></a><hr><br><h3>An Uncertainty-Driven Approach to Vortex Analysis Using Oracle Consensus and Spatial Proximity</h3><a name="An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/vortex-analysis.png" alt="vortex detection"></p>
<p>Although vortex analysis and detection have been extensively investigated in the past, none of the existing techniques are able to provide fully robust and reliable identification results. Local vortex detection methods are popular as they are efficient and easy to implement, and produce binary outputs based on a user-specified, hard threshold. However, vortices are global features, which present challenges for local detectors. On the other hand, global detectors are computationally intensive and require considerable user input. In this work, we propose a consensus-based uncertainty model and introduce spatial proximity to enhance vortex detection results obtained using point-based methods. We use four existing local vortex detectors and convert their outputs into fuzzy possibility values using a sigmoid-based soft-thresholding approach. We apply a majority voting scheme that enables us to identify candidate vortex regions with a higher degree of confidence. Then, we introduce spatial proximity- based analysis to discern the final vortical regions. Thus, by using spatial proximity coupled with fuzzy inputs, we propose a novel uncertainty analysis approach for vortex detection. We use expert’s input to better estimate the system parameters and results from two real-world data sets demonstrate the efficacy of our method.</p>
<p><a href="http://web.cse.ohio-state.edu/~biswas/template_2.pdf">Link to paper</a></p>
</p><a href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity"><small> 
Link to this post</small></a><hr><br><h3>Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation</h3><a name="Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/rotating-stall.png" alt="detection of stall"></p>
<p>Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data inmore detail. To achieve this, we use a statistical anomaly detection method to locate possible stall inception.  We further derive algorithms from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.</p>
<p><a href="http://dx.doi.org/10.1109/TVCG.2015.2467952">Link to paper</a></p>
</p><a href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation"><small> 
Link to this post</small></a><hr><br><h3>Software for Distribution Data Modeling and Visualization</h3><a name="Software-for-Distribution-Data-Modeling-and-Visualization" href="#Software-for-Distribution-Data-Modeling-and-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The <a href="https://sites.google.com/site/gravityvisdb/edda">EDDA library</a> aims at visualizing distribution data for uncertainty analysis.  </p>
<p>The goal is to provide a unified data model with generic distribution representations for the development of uncertainty visualization algorithms.  The distribution models to support will be parametric distributions like Gaussian and GMM, un-parametric distributions like histogram and KDE, as well as joint distributions.  These are encapsulated into C++ template classes.  Coupled with our experiences on developing regular and curvilinear-grid datasets in OSUFlow, we provide an API allowing to query for the distribution of a given 3D position.  The return of the query can be either the interpolated distribution or a Monte-Carlo sample of the distribution, depending on the need of the visualization algorithm.  We also provide distribution arithmetic and analysis tools including project ITL.</p>
<p>Chun-Ming Chen, <em>et al.</em> (<em>OSU</em>)</p>
</p><a href="#Software-for-Distribution-Data-Modeling-and-Visualization"><small> 
Link to this post</small></a><hr><br><h3>Uncertainty Modeling and Error Reduction for Pathline Computation in Time-varying Flow Fields</h3><a name="Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/uncertainty-modeling-1.png" alt="uncertain pathline"> <img src="images/uncertainty-modeling-2.png" alt="uncertain-pathline"></p>
<p>When the spatial and temporal resolutions of a time-varying simulation become very high, it is not possible to process or store data from every time step due to the high computation and storage cost. Although using uniformly down-sampled data for visualization is a common practice, important information in the un-stored data can be lost. Currently, linear interpolation is a popular method used to approximate data between the stored time steps. For pathline computation, however, errors from the interpolated velocity in the time dimension can accumulate quickly and make the trajectories rather unreliable. To inform the scientist the error involved in the visualization, it is important to quantify and display the uncertainty, and more importantly, to reduce the error whenever possible. In this paper, we present an algorithm to model temporal interpolation error, and an error reduction scheme to improve the data accuracy for temporally down-sampled data. We show that it is possible to compute polynomial regression and measure the interpolation errors incrementally with one sequential scan of the time-varying flow field. We also show empirically that when the data sequence is fitted with least-squares regression, the errors can be approximated with a Gaussian distribution. With the end positions of particle traces stored, we show that our error modeling scheme can better estimate the intermediate particle trajectories between the stored time steps based on a maximum likelihood method that utilizes forward and backward particle traces.</p>
<p><a href="http://dx.doi.org/10.1109/PACIFICVIS.2015.715638">Link to paper</a></p>
</p><a href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields"><small> 
Link to this post</small></a><hr><br><h3>Association Analysis for Visual Exploration of Multivariate Scientific Data Sets</h3><a name="Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/association-analysis.png" alt="association analysis"></p>
<p>The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data’s multi-faceted properties. We present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7192697">Link to paper</a></p>
</p><a href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Self-Adaptive Density Estimation of Particle Data</h3><a name="Self-Adaptive-Density-Estimation-of-Particle-Data" href="#Self-Adaptive-Density-Estimation-of-Particle-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/density-estimate.png" alt="cosmology particles and density field"></p>
<p>We conducted a study of density estimation, the conversion of discrete particle positions to a continuous field of particle density defined over a 3D Cartesian grid. The study features a methodology for evaluating the accuracy and performance of various density estimation methods, results of that evaluation for four density estimators, and a large-scale parallel algorithm for a self-adaptive method that computes a Voronoi tessellation as an intermediate step. We demonstrated the performance and scalability of our parallel algorithm on a supercomputer when estimating the density of 100 million particles over 500 billion grid points.</p>
<p>Peterka, <em>et al.</em> (<em>ANL</em>) Submitted to <em>SIAM Journal on Scientific Computing SISC Special Section on CSE15: Software and Big Data</em>, 2015.</p>
</p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data"><small> 
Link to this post</small></a><hr><br><h3>SDMAV Kick-Off PI Meeting</h3><a name="SDMAV-Kick-Off-PI-Meeting" href="#SDMAV-Kick-Off-PI-Meeting" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On Tuesday, January 13-15, there will be a Principal Investigator (PI)
Meeting, for projects funded under the Scientific Data Management,
Analysis and Visualization (SDMAV) by DOE SC ASCR, in Walnut Creek, CA.
EDDA PIs and investigators will be present for this meeting. Below
are links to download materials to be presented at this meeting for the
EDDA project.</p>
<ul>
<li><a href="files/2015-1-6/EDDA-Jan2015-Handout.docx">Handout</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Quad.ppt">Quad Chart</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Poster.pdf">Poster</a></li>
</ul>
</p><a href="#SDMAV-Kick-Off-PI-Meeting"><small> 
Link to this post</small></a><hr><br><h3>Logo Explanation</h3><a name="Logo-Explanation" href="#Logo-Explanation" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The acronym for our project is EDDA, which if you google for it you find
the description for Edda on <a href="http://en.wikipedia.org/wiki/Edda">wikipedia</a>.</p>
<blockquote>
<p>The term &quot;Edda&quot; (/ˈɛdə/; Old Norse Edda, plural Eddur) applies to 
the Old Norse Poetic Edda and Prose Edda, both of which were written 
down in Iceland during the 13th century in Icelandic, although they 
contain material from earlier traditional sources, reaching into the 
Viking Age. The books are the main sources of medieval skaldic tradition 
in Iceland and Norse mythology. </p>
</blockquote>
<p>Runic alphabets, in particular <em>futhark</em> (&quot;th&quot; is the thorn: þ, fuþark), 
was used by Scandanavian (Norse) and its use was noted use in Eddic lore.</p>
<blockquote>
<p>In Norse mythology, the runic alphabet is attested to a divine origin 
(Old Norse: reginkunnr). This is attested as early as on the Noleby 
Runestone from approximately 600 AD that reads Runo fahi raginakundo 
toj[e&#39;k]a..., meaning &quot;I prepare the suitable divine rune...&quot; and in 
an attestation from the 9th century on the Sparlösa Runestone, which 
reads Ok rað runaR þaR rægi[n]kundu, meaning &quot;And interpret the runes 
of divine origin&quot;.</p>
</blockquote>
<p>J.R.R. Tolkien used derivatives of <em>futhark</em> to describe the alphabet
used by the dwarves and even created his own called <em>Cirth</em>.</p>
<p>Well, what does this all mean for our logo? It&#39;s the transliterated
version of EDDA into <em>elder futhark</em>.</p>
<ul>
<li>E = ehwaz = &quot;M&quot; like character</li>
<li>D = dagaz = &quot;infinity&quot; like character</li>
<li>A = ansuz = &quot;F&quot; like character</li>
</ul>
</p><a href="#Logo-Explanation"><small> 
Link to this post</small></a><hr><br><h3>Introduction</h3><a name="Introduction" href="#Introduction" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>As it becomes more difficult to analyze large-scale simulation output at full
resolution, users will have to review and identify regions of interest by
transforming data into compact information descriptors that characterize
simulation results and allow detailed analysis on demand. This is because
exascale architectures will be much more constrained with respect to data
movement, and <em>in situ</em> data processing will be the norm, where the goals are to
fit the total amount of output data within a budget, to summarize and triage
data based on content, and to classify and index data to facilitate efficient
offline analysis. In addition, <em>in situ</em> analysis must be performed in a time 
and space efficient fashion, not only to avoid slowing down the simulation, but
also to not consume too much memory.  </p>
<p>Among many different feature descriptors,
the statistical information derived from data samples is a promising approach
to taming the big data avalanche, because data distributions computed from a
population can compactly describe the presence and characteristics of salient
features with minimal data movement.  The ability to computationally summarize
and process data using distributions provides an efficient and representative
capture of the information content of a large-scale data set. This
representation can adjust to size and resource constraints, with the added
benefit that uncertainty can be quantified and communicated.  </p>
<p>In this project,
<em>we posit that with the growing number of cores per node, with increasing memory
and I/O constraints in emerging extreme-scale platforms, it will be feasible
and desirable to compute distributions at simulation time, perform
memory-efficient</em> in situ <em>analysis using distributions, and save distributions
as a space-efficient summarization for on-demand, offline visualization and
analysis of salient features.</em> The key development will be a novel
distribution-based analysis and visualization framework based on in situ pro-
cessing of extreme-scale scientific data. Our goals are to ensure that
scientists can easily obtain an overview of the entire data set regardless of
the size of the simulation; understand the characteristics and locations of the
features; easily interact with the data and select regions and features of
interest; and perform all the analysis tasks with a small memory footprint.</p>
</p><a href="#Introduction"><small> 
Link to this post</small></a><hr><br></p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small><script src="javascripts/scale.fix.js"></script></body></html>