<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>EDDA: Extreme-scale Distribution-based Data Analysis</title><link rel="stylesheet" href="stylesheets/styles.css"><link rel="stylesheet" href="stylesheets/pygment_trac.css"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><!--if lt IE 9script(src='//html5shiv.googlecode.com/svn/trunk/html5.js')--></head><body><a href="#Logo-Explanation"><img src="images/edda.png" width="15%"></a><h2> <a href="https://github.com/EDDA-Project"></a>EDDA: Extreme-scale Distribution-based Data Analysis</h2><h3><a href="https://sites.google.com/site/gravityvisdb/edda">Link to EDDA Library and Open-Source Software</a></h3><h3> <a href="#Project-renewal----Introduction">Current Project (2018-2020) </a><i>DOE Program Manager: Dr. Laura Biven</i><br>Los Alamos National Laboratory, The Ohio State University,
Argonne National Laboratory<br>Jon Woodring (LANL), Han-Wei Shen (OSU), Tom Peterka (ANL)</h3><h5><a href="#Introduction">Previous project (2015-2017) </a><i>DOE Program Manager: Dr. Lucile Nowell</i><br>The Ohio State University, Argonne National Laboratory,
Los Alamos National Laboratory<br>Han-Wei Shen (OSU), Tom Peterka (ANL), Jon Woodring (LANL)<br>Gagan Agrawal (OSU), Huamin Wang (OSU), Joanne Wendelberger (LANL)</h5><h4><a href="http://science.energy.gov/ascr/">DOE SC ASCR </a>projects on the 
research for using distributions
as a proxy for large-scale data analysis.</h4><small>LA-UR-15-20108</small><hr><p><h3>End-to-End In Situ Data Processing and Analytics</h3><a name="End-to-End-In-Situ-Data-Processing-and-Analytics" href="#End-to-End-In-Situ-Data-Processing-and-Analytics" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/shen_talk.png" alt="Overview of research"></p>
<p>Professor Han-Wei Shen recently gave the keynote at Pacific Visualization 2019,
showcasing and highlighting the collaborative research in this project.</p>
<p><em>Han-Wei Shen</em>, Keynote, <em>Pacific Visualization 2019</em></p>
<p><a href="files/2019-07-29/Presentation.pdf">Link to slides</a></p>
</p><a href="#End-to-End-In-Situ-Data-Processing-and-Analytics"><small> 
Link to this post</small></a><hr><br><h3>Statistical Super Resolution for Data Analysis and Visualization of Large Scale Cosmological Simulations</h3><a name="Statistical-Super-Resolution-for-Data-Analysis-and-Visualization-of-Large-Scale-Cosmological-Simulations" href="#Statistical-Super-Resolution-for-Data-Analysis-and-Visualization-of-Large-Scale-Cosmological-Simulations" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/cosmo.png" alt="Comparison of different reconstruction methods"></p>
<p>Cosmologists build simulations for the evolution of the universe using
different initial parameters.  By exploring the datasets from different
simulation runs, cosmologists can understand the evolution of our universe and
approach its initial conditions. A cosmological simulation nowadays can
generate datasets on the order of petabytes.  Moving datasets from the
supercomputers to post data analysis machines is infeasible. We propose a
novel approach called statistical super-resolution to tackle the big data
problem for cosmological data analysis and visualization. It uses datasets
from a few simulation runs to create a prior knowledge, which captures the
relation between low- and high-resolution data.  We apply in situ statistical
down-sampling to datasets generated from simulation runs to minimize the
requirements of I/O bandwidth and storage.  High-resolution datasets are
reconstructed from the statistical down-sampled data by using the prior
knowledge for scientists to perform advanced data analysis and render
high-quality visualizations.</p>
<p><em>Ko-Chih Wang, Jiayi Xu, Jonathan Woodring, Han-Wei Shen</em></p>
<p><a href="files/2019-07-29/WangPViS19_preprint.pdf">Link to preprint paper</a></p>
<p><a href="files/2019-07-29/WangPViS19.pptx">Link to Pacific Vis 2019 presentation</a></p>
</p><a href="#Statistical-Super-Resolution-for-Data-Analysis-and-Visualization-of-Large-Scale-Cosmological-Simulations"><small> 
Link to this post</small></a><hr><br><h3>Spatial and Temporal Partitioning of Large-Scale Computer Simulations</h3><a name="Spatial-and-Temporal-Partitioning-of-Large-Scale-Computer-Simulations" href="#Spatial-and-Temporal-Partitioning-of-Large-Scale-Computer-Simulations" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/jsm_1.png" alt="Metrics for data partitioning">
<img src="images/2019-07-29/jsm_2.png" alt="Metrics for data partitioning">
<img src="images/2019-07-29/jsm_3.png" alt="Metrics for data partitioning"></p>
<p>The continual growth in size and complexity of computer simulations poses
challenges to data storage and post-processing. One solution is to partition
the simulation output, storing only a fraction of the data generated.
Previously, we developed methods for judiciously partitioning the simulated
output, both within each time step, and over time, as the simulation is
running.</p>
<p>In the partitioning framework, post-processing and analysis is performed using
only the partitioned data. Poorly-designed partitions can lead to incorrect
conclusions. We have introduced a metric and studied the parameter space to
produce partitions that closely mirror the structure of the full data,
resulting in improved understanding of the partitioning process.</p>
<p>Future work includes:</p>
<ul>
<li>Develop a method for strategically partitioning the output from large-scale
computer simulations as the simulation runs.</li>
<li>Incorporate statistical theory to identify partitioning schemes that retain
properties of the full data with decreased storage costs.</li>
<li>Illustrate the proposed method, using cosmological simulations carried out
with Argonne&#39;s HACC code.</li>
<li>Compare the partitioned data to the pre-partitioned data over multiple
time steps and across several partitioning designs, demonstrating the efficacy
of the proposed method.</li>
</ul>
<p><em>Chelsea Challacombe, Abigael Nachtsheim, Emily Casleton, Jonathan Woodring</em></p>
<p><a href="files/2019-07-29/casleton_jsm2019.pptx">Link to upcoming JSM 2019 talk</a></p>
<p><a href="files/2019-07-29/nachtsheim_partitioning_quad.pptx">Link to quad chart for future
work</a></p>
</p><a href="#Spatial-and-Temporal-Partitioning-of-Large-Scale-Computer-Simulations"><small> 
Link to this post</small></a><hr><br><h3>Exploring Machine Learning to Compress Large-Scale Time-series Sensor Data</h3><a name="Exploring-Machine-Learning-to-Compress-Large-Scale-Time-series-Sensor-Data" href="#Exploring-Machine-Learning-to-Compress-Large-Scale-Time-series-Sensor-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/shen_gross_1.png" alt="Initial results of using ML modeling to predict time series
data">
<img src="images/2019-07-29/shen_gross_2.png" alt="Initial results of using ML modeling to predict time series
data"></p>
<p>The MINOS Project (an NA-22 venture) has a massive amount of sensor data to
study the life cycle of a reactor facility. For example, with only one day of
electromagnetic sensor data, it contains 2 TB just for that sensor. It
desirable to compress this data to make the data movement, storage, and
analysis manageable.</p>
<p>Although the data appear to be unpredictable, they are not a stochastic or
random sequence-- can a machine learning model learn to predict the data, i.e.,
compress the data: Can we represent the data where the size of the model and a
subset of the data can be used to reconstruct the data at low error? In our
previous work in statistical super-resolution, we showed that this method
worked with cosmology data.</p>
<p>Thus, we continue to explore the possibility for machine learning models
(high-dimensional regression) to contribute to data compression techniques, by
integrating new machine learning methods with existing methods in signal
processing and compression.</p>
<p><em>Jingyi Shen, Robert Gross, Jonathan Woodring</em></p>
<p><a href="files/2019-07-29/summer_work_gross_shen.pptx">Link to summer presentation
slides</a></p>
</p><a href="#Exploring-Machine-Learning-to-Compress-Large-Scale-Time-series-Sensor-Data"><small> 
Link to this post</small></a><hr><br><h3>Visualization and Visual Analysis of Ensemble Data: A Survey</h3><a name="Visualization-and-Visual-Analysis-of-Ensemble-Data:-A-Survey" href="#Visualization-and-Visual-Analysis-of-Ensemble-Data:-A-Survey" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/survey.png" alt="Visualization ontology"></p>
<p>Over the last decade, ensemble visualization has witnessed a significant
development due to the wide availability of ensemble data, and the increasing
visualization needs from a variety of disciplines. From the data analysis point
of view, it can be observed that many ensemble visualization works focus on the
same facet of ensemble data, use similar data aggregation or uncertainty
modeling methods. However, the lack of reflections on those essential
commonalities and a systematic overview of those works prevents visualization
researchers from effectively identifying new or unsolved problems and planning
for further developments.</p>
<p>In this paper, we take a holistic perspective and provide a survey of ensemble
visualization. Specifically, we study ensemble visualization works in the
recent decade, and categorize them from two perspectives: (1) their proposed
visualization techniques; and (2) their involved analytic tasks. For the first
perspective, we focus on elaborating how conventional visualization techniques
(e.g., surface, volume visualization techniques) have been adapted to ensemble
data; for the second perspective, we emphasize how analytic tasks (e.g.,
comparison, clustering) have been performed differently for ensemble data. From
the study of ensemble visualization literature, we have also identified several
research trends, as well as some future research opportunities.</p>
<p><em>Junpeng Wang, Subhashis Hazarika, Cheng Li, Han-Wei Shen</em></p>
<p><a href="https://ieeexplore.ieee.org/abstract/document/8405549">Link to paper in IEEE
Xplore</a></p>
</p><a href="#Visualization-and-Visual-Analysis-of-Ensemble-Data:-A-Survey"><small> 
Link to this post</small></a><hr><br><h3>Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models</h3><a name="Uncertainty-Visualization-Using-Copula-Based-Analysis-in-Mixed-Distribution-Models" href="#Uncertainty-Visualization-Using-Copula-Based-Analysis-in-Mixed-Distribution-Models" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/copula.png" alt="Uncertainty visualizations using Copula-based
Analysis"></p>
<p>Distributions are often used to model uncertainty in many scientific datasets.
To preserve the correlation among the spatially sampled grid locations in the
dataset, various standard multivariate distribution models have been proposed
in visualization literature.  These models treat each grid location as a
univariate random variable which models the uncertainty at that location.
Standard multivariate distributions (both parametric and nonparametric) assume
that all the univariate marginals are of the same type/family of distribution.
But in reality, different grid locations show different statistical behavior
which may not be modeled best by the same type of distribution.  In this paper,
we propose a new multivariate uncertainty modeling strategy to address the
needs of uncertainty modeling in scientific datasets.</p>
<p>Our proposed method is based on a statistically sound multivariate technique
called <em>Copula</em>, which makes it possible to separate the process of estimating
the univariate marginals and the process of modeling dependency, unlike the
standard multivariate distributions.  The modeling flexibility offered by our
proposed method makes it possible to design distribution fields which can have
different types of distribution (Gaussian, Histogram, KDE etc.) at the grid
locations, while maintaining the correlation structure at the same time.
Depending on the results of various standard statistical tests, we can choose
an optimal distribution representation at each location, resulting in a more
cost efficient modeling without significantly sacrificing on the analysis
quality.  To demonstrate the efficacy of our proposed modeling strategy, we
extract and visualize uncertain features like isocontours and vortices in
various real world datasets.  We also study various modeling criterion to help
users in the task of univariate model selection.</p>
<p><em>Subhashis Hazarika, Ayan Biswas, Han-Wei Shen</em></p>
<p><a href="https://ieeexplore.ieee.org/abstract/document/8017601">Link to paper in IEEE
Xplore</a></p>
</p><a href="#Uncertainty-Visualization-Using-Copula-Based-Analysis-in-Mixed-Distribution-Models"><small> 
Link to this post</small></a><hr><br><h3>eFESTA: Ensemble Feature Exploration with Surfaces</h3><a name="eFESTA:-Ensemble-Feature-Exploration-with-Surfaces" href="#eFESTA:-Ensemble-Feature-Exploration-with-Surfaces" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2019-07-29/sde.png" alt="eFESTA"></p>
<p>Ensemble simulations are becoming prevalent in various scientific and
engineering domains, such as climate, weather, aerodynamics, and computational
fluid dynamics. An ensemble is a collection of data produced by simulations for
the same physical phenomenon conducted with different initial conditions,
parameterizations, or phenomenological models. Ensemble simulations are used to
simulate complex systems, study sensitivities to initial conditions and
parameters, and mitigate uncertainty. For example, in numerical weather
prediction, ensemble forecasts with different fore- cast models and initial
conditions are widely used to indicate the range of possible future states of
the atmosphere.</p>
<p>We propose surface density estimate (SDE) to model the spatial distribution of
surface features—isosurfaces, ridge surfaces, and streamsurfaces—in 3D ensemble
simulation data. The inputs of SDE computation are surface features represented
as polygon meshes, and no field datasets are required (e.g., scalar fields or
vector fields). The SDE is defined as the kernel density estimate of the
infinite set of points on the input surfaces and is approximated by
accumulating the surface densities of triangular patches. We also propose an
algorithm to guide the selection of a proper kernel bandwidth for SDE
computation. An ensemble Feature Exploration method based on Surface densiTy
EstimAtes (eFESTA) is then proposed to extract and visualize the major trends
of ensemble surface features. For an ensemble of surface features, each surface
is first transformed into a density field based on its contribution to the SDE,
and the resulting density fields are organized into a hierarchical
representation based on the pairwise distances between them. The hierarchical
representation is then used to guide visual exploration of the density fields
as well as the underlying surface features. We demonstrate the application of
our method using isosurface in ensemble scalar fields, Lagrangian coherent
structures in uncertain unsteady flows, and streamsurfaces in ensemble fluid
flows.</p>
<p><em>Wenbin He, Hanqi Guo, Han-Wei Shen, Tom Peterka</em></p>
<p><a href="https://ieeexplore.ieee.org/abstract/document/8525340">Link to paper in IEEE
Xplore</a></p>
</p><a href="#eFESTA:-Ensemble-Feature-Exploration-with-Surfaces"><small> 
Link to this post</small></a><hr><br><h3>Extreme-Scale Stochastic Particle Tracing for Uncertain Unsteady Flow Visualization and Analysis</h3><a name="Extreme-Scale-Stochastic-Particle-Tracing-for-Uncertain-Unsteady-Flow-Visualization-and-Analysis" href="#Extreme-Scale-Stochastic-Particle-Tracing-for-Uncertain-Unsteady-Flow-Visualization-and-Analysis" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p></p><a href="#Extreme-Scale-Stochastic-Particle-Tracing-for-Uncertain-Unsteady-Flow-Visualization-and-Analysis"><small> 
Link to this post</small></a><hr><br><h3>Parallel Partial Reduction for Large-Scale Data Analysis and Visualization</h3><a name="Parallel-Partial-Reduction-for-Large-Scale-Data-Analysis-and-Visualization" href="#Parallel-Partial-Reduction-for-Large-Scale-Data-Analysis-and-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p></p><a href="#Parallel-Partial-Reduction-for-Large-Scale-Data-Analysis-and-Visualization"><small> 
Link to this post</small></a><hr><br><h3>FY18 ASCR Report</h3><a name="FY18-ASCR-Report" href="#FY18-ASCR-Report" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p></p><a href="#FY18-ASCR-Report"><small> 
Link to this post</small></a><hr><br><h3>LANL Reverse Site Visit</h3><a name="LANL-Reverse-Site-Visit" href="#LANL-Reverse-Site-Visit" class="anchor"><span class="octicon octicon-link"></span></a><small>July 29, 2019 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p></p><a href="#LANL-Reverse-Site-Visit"><small> 
Link to this post</small></a><hr><br><h3>DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks</h3><a name="DQNViz:-A-Visual-Analytics-Approach-to-Understand-Deep-Q-Networks" href="#DQNViz:-A-Visual-Analytics-Approach-to-Understand-Deep-Q-Networks" class="anchor"><span class="octicon octicon-link"></span></a><small>August 13, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets
to train an intelligent agent that acquires optimal actions while interacting
with an environment. The model is well known for its ability to surpass
professional human players across many Atari 2600 games. Despite the superhuman
performance, in-depth understanding of the model and interpreting the
sophisticated behaviors of the DQN agent remain to be challenging tasks, due to
the long-time model training process and the large number of experiences
dynamically generated by the agent. In this work, we propose <em>DQNViz</em>, a visual
analytics system to expose details of the blind training process in four
levels, and enable users to dive into the large experience space of the agent
for comprehensive analysis. As an initial attempt in visualizing DQN models,
our work focuses more on Atari games with a simple action space, most notably
the Breakout game. From our visual analytics of the agent’s experiences, we
extract useful action/reward patterns that help to interpret the model and
control the training. Through multiple case studies conducted together with
deep learning experts, we demonstrate that DQNViz can effectively help domain
experts to understand, diagnose, and potentially improve DQN models.</p>
<h4 id="overview-of-the-dqnvis-system">Overview of the DQNVis system</h4>
<p><img src="images/2018-08-13/dqnvis.png" alt="Overview of DQNVis"></p>
<p><em>Junpeng Wang, Liang Gou, and Han-Wei Shen</em></p>
<p>* This paper was awarded Honorable Mention for Best Paper at IEEE VAST 2018.</p>
<p><a href="files/2018-08-13/dqnvis.pdf">Link to IEEE VAST 2018 pre-print copy</a></p>
</p><a href="#DQNViz:-A-Visual-Analytics-Approach-to-Understand-Deep-Q-Networks"><small> 
Link to this post</small></a><hr><br><h3>CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data</h3><a name="CoDDA:-A-Flexible-Copula-based-Distribution-Driven-Analysis-Framework-for-Large-Scale-Multivariate-Data" href="#CoDDA:-A-Flexible-Copula-based-Distribution-Driven-Analysis-Framework-for-Large-Scale-Multivariate-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>August 13, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><em>CoDDA (Copula-based Distribution Driven Analysis)</em> is a flexible framework for
large-scale multivariate datasets. A common strategy to deal with large-scale
scientific simulation data is to partition the simulation domain and create
statistical data summaries. Instead of storing the high-resolution raw data
from the simulation, storing the compact statistical data summaries results in
reduced storage overhead and alleviated I/O bottleneck. Such summaries, often
represented in the form of statistical probability distributions, can serve
various post-hoc analysis and visualization tasks. However, for multivariate
simulation data using standard multivariate distributions for creating data
summaries is not feasible. They are either storage inefficient or are
computationally expensive to be estimated in simulation time (in situ) for
large number of variables. In this work, using copula functions, we propose a
flexible multivariate distribution-based data modeling and analysis framework
that offers significant data reduction and can be used in an in situ
environment. Using the proposed multivariate data summaries, we perform various
multivariate post-hoc analyses like query-driven visualization and
sampling-based visualization. We evaluate our proposed method on multiple
real-world multivariate scientific datasets.  To demonstrate the efficacy of
our framework in an in situ environment, we apply it on a large-scale flow
simulation.</p>
<h4 id="codda-workflow">CoDDA workflow</h4>
<p><img src="images/2018-08-13/codda_1.png" alt="CoDDA workflow"></p>
<h4 id="comparing-hurricane-isabel-pressure-reconstruction-methods">Comparing Hurricane Isabel pressure, reconstruction methods</h4>
<p><img src="images/2018-08-13/codda_2.png" alt="Comparing Hurricane Isabel"></p>
<ul>
<li>(a) original ground truth</li>
<li>(b) multivariate histograms</li>
<li>(c) Gaussian Mixture Model with 3 modes</li>
<li>(d) CoDDA</li>
</ul>
<p><em>Authors go here</em></p>
<p><a href="files/2018-08-13/codda.pdf">Link to IEEE SciVis 2018 pre-print copy</a></p>
</p><a href="#CoDDA:-A-Flexible-Copula-based-Distribution-Driven-Analysis-Framework-for-Large-Scale-Multivariate-Data"><small> 
Link to this post</small></a><hr><br><h3>Dynamic Load Balancing for Parallel Particle Tracing</h3><a name="Dynamic-Load-Balancing-for-Parallel-Particle-Tracing" href="#Dynamic-Load-Balancing-for-Parallel-Particle-Tracing" class="anchor"><span class="octicon octicon-link"></span></a><small>August 10, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2018-08-10/Picture17.png" alt="Dynamic Particle Redistribution"></p>
<p>In flow visualization and analysis, particle tracing is a fundamental technique
for visualizing and analyzing flow fields. By tracing particles in the data
domain, users can conduct many applications, for example, generating
streamlines or pathlines, tracking particles from source to destination regions
for source-destination queries, or computing FTLE fields to characterize the
boundary of flow fields. In particle tracing, we always need to handle large
data and the computational costs are expensive, so scalable and parallel
solutions are needed.</p>
<p>However, it is likely that the workload is imbalanced.  We proposed two
solutions to address these problems. The first one is a dynamic load-balancing
method based on data repartitioning for parallel particle tracing, as shown in
top-half of the Figure. During run time, we periodically perform data
repartitioning to balance the workload of each process based on the estimation
of the block workload.</p>
<p>We studied this data-repartitioning method with different data sets on Vesta, a
Blue Gene/Q supercomputer at Argonne National Laboratory. Compared with other
load-balancing algorithms, our method does not need any preprocessing on the
raw data and does not require any dedicated process for work scheduling, while
it has the capability to balance uneven workload efficiently. The performance
study shows improved load balance and high efficiency of this method on tracing
particles in both steady and unsteady flow.</p>
<p>However, the this method still requires data block movement during run time,
which makes it difficult to scale up. This observation prompted us to design a
load-balancing algorithm from the perspective of particle distribution.
Therefore, we proposed the second method, a dynamically load-balanced algorithm
using k-d (short for k-dimensional) trees, which uses k-d tree decomposition to
periodically evenly redistribute particles across processes for load balancing.
Our final solution is a novel redesign of k-d tree decomposition, namely, the
constrained k-d tree, to redistribute particles in the data-parallel particle
tracing, as shown in bottom-half of the Figure.</p>
<p>This constrained k-d tree method has also been evaluated with various flow
visualization and analysis tasks on Vesta. With up to 8K parallel processes, we
demonstrated that compared with the baseline data-parallel particle tracing
method, our constrained k-d tree approach significantly improves the
performance in both load balancing and scalability.</p>
<p><strong>[1]</strong> Jiang Zhang, Hanqi Guo, Xiaoru Yuan, and Tom Peterka.
<em>Dynamic Data Repartitioning for Load-Balanced Parallel Particle Tracing.</em>
In Proceedings of IEEE Pacific Visualization Symposium (PacificVis &#39;18),
pages 86-95, Kobe, Japan, April 10-13, 2018.</p>
<p><strong>[2]</strong> Jiang Zhang, Hanqi Guo, Fan Hong, Xiaoru Yuan, and Tom Peterka.
<em>Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for
Parallel Particle Tracing.</em>
IEEE Transactions on Visualization and Computer Graphics (VIS &#39;17),
24(1):954-963, 2018.</p>
<p><a href="files/2018-08-10/dynamic_load_balancing.docx">Link to summary Word doc</a></p>
</p><a href="#Dynamic-Load-Balancing-for-Parallel-Particle-Tracing"><small> 
Link to this post</small></a><hr><br><h3>Statistical-based Super-resolution for Cosmological Emulator</h3><a name="Statistical-based-Super-resolution-for-Cosmological-Emulator" href="#Statistical-based-Super-resolution-for-Cosmological-Emulator" class="anchor"><span class="octicon octicon-link"></span></a><small>August 10, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><em>Preliminary Results</em></p>
<p>In investigating other methods to improve the quality and size reduction of
<a href="#Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study">Gaussian Mixture Modeling (GMM)</a>
for cosmological simulations, we have studied the application of
<em>super-resolution</em> techniques applied to our statistical GMM modeling. In
machine learning, <em>super-resolution</em> methods use image training techniques
to train a dictionary of high-resolution imagery. This allows image databases
to reconstruct high-resolution images from low-resolution samples using the
dictionary training set, to infer high-resolution information from
low-resolution data.</p>
<p>In our work, we investigated super-resolution reconstruction of
<a href="https://github.com/AMReX-Astro/Nyx">Nyx</a> simulation data
by mapping low-resolution GMM model data into a high-resolution Nyx dictionary.
This allows us both to reduce the GMM model training time, by building a lower
resolution GMM model, and still retain low reconstruction error and high
precision through the super-resolution dictionary.</p>
<p>Our workflow for applying the super-resolution method to scientific data follows:</p>
<ol>
<li>Run Nyx for several input parameters</li>
<li>Use the high-resolution data from Nyx to generate a high-resolution dictionary
(statistical prior knowledge)</li>
<li>Run Nyx for the rest of the input parameter space, but down-sample the data
into low-resolution histogram data (i.e., block-wise histograms)</li>
<li>Train a GMM model on the coarse resolution data from step (3)</li>
<li>Reconstruct high resolution data by low-resolution GMM reconstruction and
super-resolution lookup and interpolation, using data from step (2) and (4)</li>
</ol>
<p>Our preliminary results show that we are able to simultaneously achieve low
storage sizes of approximately 3% of the original data size, and low RMSE
on average of 0.5% error for density and z-momentum, with 2% error on temperature.
This method achieves better space savings and reconstruction error than plain
GMM modeling of Nyx.</p>
<h4 id="super-resolution-workflow">Super resolution workflow</h4>
<p><img src="images/2018-08-10/Picture14.png" alt="Super resolution workflow"></p>
<h4 id="examples-of-super-resolution-reconstruction-original-left-reconstructed-right-">Examples of super-resolution reconstruction (Original left, Reconstructed right)</h4>
<p><img src="images/2018-08-10/Picture15.png" alt="Super-resolution density">
<img src="images/2018-08-10/Picture16.png" alt="Super resolution temperature"></p>
<p><em>Ko-Chih Wang, Zarija Lukic, and Jon Woodring</em></p>
<p><a href="files/2018-08-10/OneSlideSR.pptx">Link to poster with original resolution images</a></p>
</p><a href="#Statistical-based-Super-resolution-for-Cosmological-Emulator"><small> 
Link to this post</small></a><hr><br><h3>Incremental Training of Cosmological Emulator</h3><a name="Incremental-Training-of-Cosmological-Emulator" href="#Incremental-Training-of-Cosmological-Emulator" class="anchor"><span class="octicon octicon-link"></span></a><small>August 10, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><em>Preliminary Results</em></p>
<p><a href="#Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study">Gaussian Mixture Modeling (GMM)</a>
of large-scale cosmological data sets can take a long time, due to that typical
GMM methods will attempt to train the data in one go. This requires saving all
of the intermediate data from simulations, and then constructing (training) the
GMM model. This is inefficient, especially for petascale and exascale
supercomputers, as the GMM modeling requires massive amounts of memory and
storage. To reduce the I/O burden of constructing the GMM model in the first
place, we are investigating building the GMM model through <em>incremental
training.</em></p>
<p>Incremental GMM training builds the data model by sampling an existing GMM
model, adding new data, and then retraining on that data set. An example workflow
would be:</p>
<ol>
<li>Run <a href="https://github.com/AMReX-Astro/Nyx">Nyx</a> for several input parameters</li>
<li>Train a GMM model on Nyx&#39;s output data</li>
<li>Run Nyx for several more input parameters</li>
<li>Sample the GMM model and add the new data from step (3)</li>
<li>Train a new GMM model on the data from step (4)</li>
<li>Go to step (3)</li>
</ol>
<p>This amortizes the cost of building a statistical GMM model for simulations,
by spreading the I/O cost and GMM building cost through incremental steps. In our
initial studies, we achieve approximately 0.64% RSME error on reconstruction
when allowing the number of Gaussian parameters to grow. If we keep the number
of Gaussians to a fixed number, such as 5 Gaussians, the errors are larger,
but within a tolerable range of 1% RSME.</p>
<p>In future work, we will investigate the application of <em>in situ</em> GMM training,
as our incremental method would allow for the integration of GMM methods
directly into simulations. This would bypass the need for post-processing GMM
training, reducing the storage needs, and building the GMM model as the
simulation runs, allowing for faster scientific analysis turn-around times.</p>
<h4 id="incremental-gmm-reconstruction-x-axis-incremental-simulation-run-y-axis-rmse-">Incremental GMM reconstruction (x-axis incremental simulation run, y-axis RMSE)</h4>
<p><img src="images/2018-08-10/Picture11.png" alt="Incremental training">
<img src="images/2018-08-10/Picture12.png" alt="Incremental training">
<img src="images/2018-08-10/Picture13.png" alt="Incremental training"></p>
<p><em>Jiayi Xu, Zarija Lukic, and Jon Woodring</em></p>
<p><a href="files/2018-08-10/OneSlideSR.pptx">Link to poster with original resolution images</a></p>
</p><a href="#Incremental-Training-of-Cosmological-Emulator"><small> 
Link to this post</small></a><hr><br><h3>Emulator of Cosmological Simulation for Initial Parameters Study</h3><a name="Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study" href="#Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study" class="anchor"><span class="octicon octicon-link"></span></a><small>August 10, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><em>Preliminary Results</em></p>
<p><a href="https://github.com/AMReX-Astro/Nyx">Nyx</a>, a DOE cosmological model,
simulates the evolution of the large-scale structures in the universe,
given the &quot;initial conditions&quot; of the universe. Though, it is not clear
what initial conditions match the observable universe, thus part of the
science is to find the parameterization of the simulations that best
match the observed universe.</p>
<p>The cosmologists are typically interested in three to
ten of these initial parameters of interest, which creates huge parameter space
to search for the best matching conditions. With high-resolution simulations,
each simulation will generate multiple quantities of interest, on a high-resolution
grid, such as 4096^3. These simulations generate massive amounts of data
and performing data analysis will be slow, due to the limited I/O bandwidth
compared to the amount of raw data, in addition to limited storage capabilities.</p>
<p>To tackle the problem of analysis of massive data for cosmological studies,
we are studying techniques to reduce the overall data size, but keep
enough precision for high-fidelity statistical analysis. We have
developed a preliminary <em>Gaussian Mixture Model-based (GMM) Emulator</em>
to achieve our goal. The GMM-based Emulator uses multi-variate GMM methods
to compactly summarize the data from simulations by modeling the cosmological
data with Gaussian distributions. This allows the cosmologists to reconstruct
the data for any set of initial conditions, as well as predict the data for
simulations that have not been run yet. With 5 Gaussians, we are able to
reduce the data to 7.5% of its original size with RMSE within the 1% error
regime, which is acceptable for statistical studies of cosmological data.</p>
<h4 id="reconstruction-error-graphs">Reconstruction Error Graphs</h4>
<p><img src="images/2018-08-10/Picture1.png" alt="Error graph step 50">
<img src="images/2018-08-10/Picture2.png" alt="Error graph step 100">
<img src="images/2018-08-10/Picture3.png" alt="Error graph step 150">
<img src="images/2018-08-10/Picture4.png" alt="Error graph step 200"></p>
<h4 id="volume-rendering-reconstructed-images-original-left-reconstructed-right-">Volume Rendering Reconstructed Images (Original left, Reconstructed right)</h4>
<p><img src="images/2018-08-10/Picture5.png" alt="Original image Density">
<img src="images/2018-08-10/Picture6.png" alt="Reconstructed image Density"></p>
<p><img src="images/2018-08-10/Picture7.png" alt="Original image Temperature">
<img src="images/2018-08-10/Picture8.png" alt="Reconstructed image Temperature"></p>
<p><img src="images/2018-08-10/Picture9.png" alt="Original image z-Momentum">
<img src="images/2018-08-10/Picture10.png" alt="Reconstructed image z-Momentum"></p>
<p><em>Jiayi Xu, Ko-Chih Wang, Zarija Lukic, and Jon Woodring</em></p>
<p><a href="files/2018-08-10/OneSlideSR.pptx">Link to poster with original resolution images</a></p>
</p><a href="#Emulator-of-Cosmological-Simulation-for-Initial-Parameters-Study"><small> 
Link to this post</small></a><hr><br><h3>Information Guided Data Sampling and Recovery using Bitmap Indexing</h3><a name="Information-Guided-Data-Sampling-and-Recovery-using-Bitmap-Indexing" href="#Information-Guided-Data-Sampling-and-Recovery-using-Bitmap-Indexing" class="anchor"><span class="octicon octicon-link"></span></a><small>April 5, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2018-4-5/wei_1.png" alt="Sampling and Recovery with Bitmap Indexing"></p>
<p>Creating a data representation is a common approach for efficient and effective
data management and exploration. The compressed bitmap indexing is one of the
emerging data representation used for large-scale data exploration. Performing
sampling on the bitmapindexing based data representation allows further
reduction of storage overhead and be more flexible to meet the requirements of
different applications. In this paper, we propose two approaches to solve two
potential limitations when exploring and visualizing the data using
sampling-based bitmap indexing data representation.  First, we propose an
adaptive sampling approach called information guided stratified sampling
(IGStS) for creating compact sampled datasets that preserves the important
characteristics of the raw data.  Furthermore, we propose a novel data recovery
approach to reconstruct the irregular subsampled dataset into a volume dataset
with regular grid structure for qualitative post-hoc data exploration and
visualization. The quantitative and visual efficacy of our proposed data
sampling and recovery approaches are demonstrated through multiple experiments
and applications.</p>
<p><em>Tzu-Hsuan Wei, Soumya Dutta, and Han-Wei Shen</em></p>
</p><a href="#Information-Guided-Data-Sampling-and-Recovery-using-Bitmap-Indexing"><small> 
Link to this post</small></a><hr><br><h3>Image and Distribution Based Volume Rendering for Large Data Sets</h3><a name="Image-and-Distribution-Based-Volume-Rendering-for-Large-Data-Sets" href="#Image-and-Distribution-Based-Volume-Rendering-for-Large-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>April 5, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2018-4-5/wang_1.png" alt="Overview of the system"></p>
<p><img src="images/2018-4-5/wang_2.png" alt="Isabel -- left original, right proxy">
<img src="images/2018-4-5/wang_3.png" alt="Turbine -- left original, right proxy"></p>
<p>Analyzing scientific datasets created from simulations on modern supercomputers
is a daunting challenge due to the fast pace at which these datasets continue
to grow. Low cost post analysis machines used by scientists to view and analyze
these massive datasets are severely limited by their deficiencies in storage
bandwidth, capacity, and computational power. Trying to simply move these
datasets to these platforms is infeasible. Any approach to view and analyze
these datasets on post analysis machines will have to effectively address the
inevitable problem of data loss. Image based approaches are well suited for
handling very large datasets on low cost platforms.  Three challenges with
these approaches are how to effectively represent the original data with
minimal data loss, analyze the data in regards to transfer function
exploration, which is a key analysis tool, and quantify the error from data
loss during analysis. We present a novel image based approach using
distributions to preserve data integrity. At each view sample, view dependent
data is summarized at each pixel with distributions to define a compact proxy
for the original dataset. We present this representation along with how to
manipulate and render large scale datasets on post analysis machines. We show
that our approach is a good trade off between rendering quality and interactive
speed and provides uncertainty quantification for the information that is lost.</p>
<p><em>Ko-Chih Wang, et al.</em></p>
</p><a href="#Image-and-Distribution-Based-Volume-Rendering-for-Large-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration</h3><a name="An-Automatic-Deformation-Approach-for-Occlusion-Free-Egocentric-Data-Exploration" href="#An-Automatic-Deformation-Approach-for-Occlusion-Free-Egocentric-Data-Exploration" class="anchor"><span class="octicon octicon-link"></span></a><small>April 5, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2018-4-5/cheng_1.png" alt="Change in Viewpoint"></p>
<p>Occlusion management is an important task for three dimension data exploration.
For egocentric data exploration, the occlusion problems, caused by the camera
being too close to opaque data elements, have not been well addressed by
previous studies. In this paper, we propose an automatic approach to resolve
these problems and provide an occlusion free egocentric data exploration. Our
system utilizes a state transition model to monitor both the camera and the
data, and manages the initiation, duration, and termination of deformation with
animation. Our method can be applied to multiple types of scientific datasets,
including volumetric data, polygon mesh data, and particle data. We demonstrate
our method with different exploration tasks, including camera navigation,
isovalue adjustment, transfer function adjustment, and time varying
exploration. We have collaborated with a domain expert and received positive
feedback.</p>
<p><em>Cheng Li, Joachim Moortgat, Han-Wei Shen</em></p>
</p><a href="#An-Automatic-Deformation-Approach-for-Occlusion-Free-Egocentric-Data-Exploration"><small> 
Link to this post</small></a><hr><br><h3>In Situ Prediction Driven Feature Analysis in Jet Engine Simulations</h3><a name="In-Situ-Prediction-Driven-Feature-Analysis-in-Jet-Engine-Simulations" href="#In-Situ-Prediction-Driven-Feature-Analysis-in-Jet-Engine-Simulations" class="anchor"><span class="octicon octicon-link"></span></a><small>April 5, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2018-4-5/dutta_1.png" alt="Visual Analytics System"></p>
<p><img src="images/2018-4-5/dutta_2.png" alt="Fuzzy Rules Extraction"></p>
<p>Efficient feature exploration in large-scale data sets using traditional
post-hoc analysis approaches is becoming prohibitive due to the bottleneck
stemming from I/O and output data sizes. This problem becomes more challenging
when an ensemble of simulations are required to run for studying the influence
of input parameters on the model output. As a result, scientists are inclining
more towards analyzing the data in situ while it resides in the memory. In situ
analysis aims at minimizing expensive data movement while maximizing the
resource utilization for extraction of important information from the data. In
this work, we study the evolution of rotating stall in jet engines using data
generated from a large-scale flow simulation under various input conditions.
Since the features of interest lack a precise descriptor, we adopt a fuzzy
rule-based machine learning algorithm for efficient and robust extraction of
such features. For scalable exploration, we advocate for an off-line learning
and in situ prediction driven strategy that facilitates in-depth study of the
stall. Task-specific information estimated in situ is visualized interactively
during the post-hoc analysis revealing important details about the inception
and evolution of stall. We verify and validate our method through comprehensive
expert evaluation demonstrating the efficacy of our approach.</p>
<p><em>Soumya Dutta, Han-Wei Shen, Jen-Ping Chen</em></p>
<p><a href="http://web.cse.ohio-state.edu/~dutta.33/pvis2018_fuzzy_insitu.pdf">Link to pre-print</a></p>
</p><a href="#In-Situ-Prediction-Driven-Feature-Analysis-in-Jet-Engine-Simulations"><small> 
Link to this post</small></a><hr><br><h3>Project renewal -- Introduction</h3><a name="Project-renewal----Introduction" href="#Project-renewal----Introduction" class="anchor"><span class="octicon octicon-link"></span></a><small>April 5, 2018 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><em>Our project has been renewed for three more years under the DOE ASCR Proposal
 titled &quot;Visual Analytics for Large Scale Scientific Ensemble Datasets&quot;.</em></p>
<p>Scientific ensemble data sets have played increasingly more important roles for
uncertainty quantification in various scientific and engineering domains, such
as climate, weather, aerodynamics, and computational fluid dynamics.  Ensembles
are collections of data produced by simulations or experiments conducted with
different initial conditions, parameterizations, or phenomenological models.
They are usually used to describe complex systems, study sensitivities to
initial conditions and parameters, and mitigate uncertainty. The goal of this
proposal is to develop visual analytic techniques for large scale scientific
ensemble data sets. Using ensemble simulations as an example, for a single run
of such a simulation, there can be data generated in the range of several
hundred gigabytes to tens of terabytes. A large scale ensemble dataset can
consist of hundreds or thousands of such instances, with many variables in the
form of scalar, vector, or tensor, and has a large number of samples in the
high-dimensional input parameter space.</p>
<p>This project aims to build a comprehensive visual analytic framework for
analyzing large scale scientific ensemble data. Our framework will provide a
strong foundation for developing future generation visualization techniques for
a very important class of applications in a wide range of scientific
disciplines. The development of our integrated analysis and visualization
framework will not only make tangible contribution to our target applications,
but can also be generalized to other domain problems. The key impact that we
anticipate is the demonstration of a working and attractive solution to assist
scientists to comprehend vast amounts of data generated by large-scale ensemble
applications.</p>
</p><a href="#Project-renewal----Introduction"><small> 
Link to this post</small></a><hr><br><h3>FY16 &amp; first half of FY17 Progress and Updates</h3><a name="FY16-&amp;-first-half-of-FY17-Progress-and-Updates" href="#FY16-&amp;-first-half-of-FY17-Progress-and-Updates" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On March 14-16, we will be having a project PI meeting 
at the Hyatt in Bethesda, MD. Below are links to the
handout, poster, and quad chart presenting the update
material on the EDDA project that will be presented at
the meeting.</p>
<p><a href="files/2017-3-13/handout_ascr_osu.docx">Handout</a> |
<a href="files/2017-3-13/poster_ascr_osu.ppt">Poster</a> |
<a href="files/2017-3-13/quad_ascr_osu.ppt">Quad Chart</a></p>
<p>Detailed updates are contained in the following posts,
with a link to the first in the set.</p>
<p><a href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets">Direct link to the first</a></p>
</p><a href="#FY16-&amp;-first-half-of-FY17-Progress-and-Updates"><small> 
Link to this post</small></a><hr><br><h3>pAIC: Comparing Partitioning Metrics on Three Datasets</h3><a name="pAIC:-Comparing-Partitioning-Metrics-on-Three-Datasets" href="#pAIC:-Comparing-Partitioning-Metrics-on-Three-Datasets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/paic1.png" alt="pAIC">
<img src="images/2017-3-13/paic2.png" alt="pAIC"></p>
<p><img src="images/2017-3-13/paic3.png" alt="pAIC"></p>
<p>Data partitioning becomes necessary when a large-scale simulation produces more
data than can be feasibly stored.  The goal is to partition the data, typically
so that every element belongs to one and only one partition, and store summary
information about the partition, either a representative value plus an estimate
of the error or a distribution.  Once the partitions are determined and the
summary information stored, the raw data is discarded. This process can be
performed in situ, meaning while the simulation is running. </p>
<p>When creating the partitions there are many decisions that must be made.  For
instance, how to determine once an adequate number of partitions have been
created, how are the partitions created with respect to dividing the data, or
how many variables should we consider simultaneously.  In addition, decisions
must be made for how to summarize the information within each partition.
Because of the combinatorial number of possible ways to partition and summarize
the data, a method of comparing the different possibilities will help guide
researchers into choosing a good partitioning and summarization scheme for
their application. In this work we will present a metric (pAIC)
that was created to balance the tradeoff between accuracy and storage cost. </p>
<p>Chelsea Challacombe, Emily Casleton, Joanne Wendelberger, 
and Jon Woodring (LANL)</p>
</p><a href="#pAIC:-Comparing-Partitioning-Metrics-on-Three-Datasets"><small> 
Link to this post</small></a><hr><br><h3>Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plot (NPCP)</h3><a name="Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)" href="#Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/npcp1.png" alt="Nested Parallel Coordinate Plot"></p>
<p><img src="images/2017-3-13/npcp2.png" alt="Nested Parallel Coordinate Plot"></p>
<p><strong>Problem</strong>: This work studies the input parameters (multi-resolution
convective parameters) of multi-resolution climate simulations. Domain experts
are specifically interested in: the correlations between parameters in the same
resolution (intra-resolution correlation visualization) and the difference
between parameters’ correlations in different resolutions (inter-resolution
correlation comparison). It is also critical to build the connection between
the multi-resolution convective parameters and the large spatial-temporal
climate ensemble outputs.</p>
<p><strong>Proposed Solution</strong>: Based on the requirements from domain experts, we
propose an augmented design of parallel coordinates plots, called Nested
Parallel Coordinates Plot (NPCP), to visualize the multi-resolution convective
parameters in climate modeling. This new design of PCP has been integrated into
a visual analytics system, which is equipped with multiple coordinated views,
to help domain scientists build the connection between complex input parameter
settings and spatial temporal ensemble outputs.</p>
<p><a href="files/2017-3-13/climate_npcp.pptx">Link to the slide</a></p>
</p><a href="#Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)"><small> 
Link to this post</small></a><hr><br><h3>Virtual Retractor: An Interactive Data Exploration System Using Physically Based Deformation</h3><a name="Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation" href="#Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/retractor1.png" alt="Virtual Retractor"></p>
<p><img src="images/2017-3-13/retractor2.png" alt="Virtual Retractor"></p>
<p><strong>Problem</strong>: Interactive data exploration plays a fundamental role in analyzing
three dimensional scientific data. Occlusion management and feature
preservation are among the key factors to ensure effective identification and
extraction of three-dimensional features. Existing methods may not able to
combine the occlusion removal task, together with preserving features flexibly
defined by data properties, while providing interactions with real-time
performance.</p>
<p><strong>Proposed Solution</strong>: we propose a new data exploration system that allows
direct manipulation of data with cutting and splitting capabilities. The
procedure is carried out by deforming a tetrahedral mesh that has a void in the
middle to simulate the incision created by the cut. The splitting operation
enlarges the void to allow users to observe the inner structure which was
originally occluded. Our mesh is constructed with the local data properties
taken into account, such as local data density or gradient. Therefore the
deformation will affected by the selected data property. Regions with high data
property values are more solid and harder to be deformed, while regions with
lower property values will be deformed more. Therefore the deformation can keep
interesting features of the data.</p>
<p><a href="files/2017-3-13/virtual_retractor.pptx">Link to the slide</a></p>
</p><a href="#Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation"><small> 
Link to this post</small></a><hr><br><h3>Range Likelihood Tree: A Compact and Effective Representation for Visual Exploration of Uncertain Data Sets</h3><a name="Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets" href="#Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/range_tree1.png" alt="Range Likelihood Tree"></p>
<p><img src="images/2017-3-13/range_tree2.png" alt="Range Likelihood Tree"></p>
<p><img src="images/2017-3-13/range_tree3.png" alt="Range Likelihood Tree"></p>
<p><strong>Problem</strong>: Uncertain data visualization plays a fundamental role in many
applications such as weather forecast and analysis of fluid flows. Exploring
scalar uncertain data modeled as probability distribution fields is a
challenging task because the underlying features are often more complex, and
the data associated with each grid point are high dimensional.</p>
<p><strong>Proposed Solution</strong>: In this work, we present a compact and effective
representation called Range Likelihood Tree (RLT), to summarize and explore
probability distribution fields. The key idea is to consider the different
roles that subranges (subspaces of the value domain) may play in understanding
probability distributions, and decompose and summarize each complex probability
distribution over a few representative subranges by cumulative probabilities.
In our method, the value domain is first partitioned into subranges, then the
distribution at each grid point is transformed according to the cumulative
probabilities of the point’s distribution in those subranges. Organizing the
subranges into a hierarchical structure based on how these cumulative
probabilities are spatially distributed in the grid points, the new range
likelihood tree representation allows effective classification and
identification of features through user query and exploration. We present an
exploration framework with multiple interactive views to explore probability
distribution fields, and provide guidelines for visual exploration using our
framework.</p>
<p><a href="files/2017-3-13/range_tree.pptx">Link to the slide</a></p>
</p><a href="#Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Homogeneity Guided Probabilistic Data Summaries for Analysis and Visualization of Large-Scale Data Sets</h3><a name="Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets" href="#Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/hguided1.png" alt="Homogeneity Guided Probabilistic Data Summaries"></p>
<p><img src="images/2017-3-13/hguided2.png" alt="Homogeneity Guided Probabilistic Data Summaries"></p>
<p><strong>Problem</strong>: </p>
<ul>
<li>Existing region-based statistical data summaries rely upon regular
partitioning  that results in partitions with high data value variation and
uncertainty leading to increased sampling error is analysis </li>
<li>Visualizations produced from these statistical data summarizations introduce
artifacts and distortions making visual analysis less effective</li>
</ul>
<p><strong>Solution</strong>: </p>
<ul>
<li>We propose a homogeneous region guided data partitioning which employs a fast
clustering algorithm SLIC for in situ partition generation </li>
<li>SLIC based homogeneous partitions are summarized using a hybrid distribution
scheme of either a single Gaussian or mixture of Gaussians per partition</li>
<li>Extensive quantitative and qualitative comparisons of our method with regular
partitioning and k-d tree based partitioning reveal that our method is
superior in quality vs storage trade-off and can be performed in situ in a
scalable way </li>
</ul>
<p><a href="files/2017-3-13/homogenity_guided.pptx">Link to the slide</a></p>
</p><a href="#Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Statistical Visualization and Analysis of Large Data Using a Value-based Spatial Distribution</h3><a name="Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution" href="#Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/value_space1.png" alt="Value-based Spatial Distribution"></p>
<p><img src="images/2017-3-13/value_space2.png" alt="Value-based Spatial Distribution"></p>
<p><strong>Problem</strong>: The computational power of modern supercomputers allow scientists
to model physical phenomena with high-resolution simulation. However, analyzing
such large-scale scientific simulation data is challenging due to the
incompatibility between memory limitations, I/O capacities, and high
computational power. Using distribution-based representation to handle big data
sets becomes popular, but the distribution inherently lacks the spatial
information of samples and causes the low visualization quality. Developing the
technique to improve visualization quality from the distribution based
representation is necessary.</p>
<p><strong>Proposed Solution</strong>: In addition to the traditional value distribution, we
construct and store the spatial distributions where the locations of samples
are collected and stored as a multi-dimensional distribution for each value
sub-range. Each multi-dimensional distribution is stored using compact
distribution representation which is Spatial Gaussian Mixture Model (GMM). The
Spatial GMM maps the locations of the data points in different value ranges to
probabilities. When visualizing the data set, we utilize our representation to
infer the probability for a value to reside at arbitrary location using Bayes’
rule, which combines known information (the value distribution) and  additional
evidences (the Spatial GMMs) from a given condition. Equipped with this spatial
information, our approach produces lower variance, and hence lower uncertainty,
in the results of statistical based analysis and visualizations.</p>
<p><a href="files/2017-3-13/spatial_distribution.pptx">Link to the slide</a></p>
</p><a href="#Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution"><small> 
Link to this post</small></a><hr><br><h3>Visualizing the Variations of Ensemble Isosurfaces</h3><a name="Visualizing-the-Variations-of-Ensemble-Isosurfaces" href="#Visualizing-the-Variations-of-Ensemble-Isosurfaces" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/vis_variations.png" alt="Variations of Ensemble Isosurfaces"></p>
<p><strong>Goal</strong>: </p>
<ul>
<li>To visualize the variation of ensemble surfaces for a range of isovalues.</li>
</ul>
<p><strong>Implementation</strong>: </p>
<ul>
<li>Visualize the isosurface order statistics using interactive PCP</li>
<li>Color mapped surface visualization showing the distance of a member from the
median surface.  </li>
<li>Volume rendered image of the distance of each point to the median surface. </li>
</ul>
<p><a href="files/2017-3-13/vis_variations.pptx">Link to the slide</a></p>
</p><a href="#Visualizing-the-Variations-of-Ensemble-Isosurfaces"><small> 
Link to this post</small></a><hr><br><h3>Efficient Distribution-based Feature Search in Multi-field Datasets</h3><a name="Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets" href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/dist_feature1.png" alt="Distribution-Based Feature Search"></p>
<p><img src="images/2017-3-13/dist_feature2.png" alt="Distribution-Based Feature Search"></p>
<p><strong>Problem</strong>: Distribution-based (histogram-based) features has been utilized in
many volume analysis and visualization applications. However, local histogram
computation and matching is difficult in multi-field dataset due to the high
computational cost. 1. It’s infeasible to scan through the entire data space
and compute and compare local histogram in each location. 2. The number of
histogram bins increases exponentially as the number of fields or dimensions
increases, which requires a large amount of bin-by-bin comparison. 3. The high
computation cost when searching for large-sized feature, which is defined by a
large neighborhood-sized histogram.</p>
<p><strong>Proposed Solution</strong>: We utilizing bitmap indexing to reduce the search space
from the entire space domain to the voxels whose values fall into the
user-defined value range and their neighborhood voxels. Then apply the local
deposit approach to construct a local histogram in an inverse way. In the
multi-field feature search cases, we proposed two complementary algorithms for
accelerating local distribution searches. Both algorithms first approximate a
search result, and then use a low-cost refinement step to generate the final
search result. The first approach is merged-bin comparison (MBC). Instead of
comparing individual bins iteratively, we compare multiple histogram bins
between two histograms in one pass. Utilizing a property of distance measures,
our approximate search result from MBC has no false negatives so that the
refinement process only needs to remove the false positives to generate the
final result. The second approach is called sampled-active-voxels (SAV). This
utilizes stratified sampling to quickly generate approximate initial results,
which are close to the final results when compared to simple random sampling.
So the cost of refinement can thus be reduced.</p>
<p><a href="files/2017-3-13/distribution_feature.pptx">Link to the slide</a></p>
</p><a href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets"><small> 
Link to this post</small></a><hr><br><h3>Scoreboard: Designing a Platform for Comparing Triage Algorithms</h3><a name="Scoreboard:-Designing-a-Platform-for-Comparing-Triage-Algorithms" href="#Scoreboard:-Designing-a-Platform-for-Comparing-Triage-Algorithms" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/scoreboard1.png" alt="Scoreboard"></p>
<p><img src="images/2017-3-13/scoreboard2.png" alt="Scoreboard"></p>
<p><strong>Scoreboard</strong>: A community for comparing the performance of different data
triage, sampling, and summarization algorithms.</p>
<p><strong>Purpose</strong>:
Since it is difficult to know how data reduction algorithms will
perform on data, we describe Scoreboard, a resource for sharing data
and triage algorithms in the scientific community.
The purpose of this document is to present the detailed design choices and
strategies that will be used during the development of Scoreboard. It is a
web-based system that will provide the users a set of tools for sharing the
performance of various data triage, compression, and sampling algorithms 
over the entire parameter space. This will allow application scientists
and computer scientists to collaborate on algorithms and data for
large-scale data triage.</p>
<p><strong>Features</strong>: Scoreboard will be built on a web publishing system where the
results will be publically available. The users will be provided with an
interactive interface where they will be able to upload their testing
algorithms and the parameter list on which they expect to run the algorithms.
The results of different runs along with the parameter set used will be stored
in a database, showing how their algorithm performs on different data sets. </p>
<p>After the completion of the run, the users, domain scientists, and computer
scientists will be able to analyze the performance of the algorithms in a
side-by-side fashion by performing queries the shared data. Scoreboard will
have one or more associated error metrics, which will indicate the goodness of
the triage algorithms. Hence, the users will be able to compare and contrast
different sampling, compression, and triage algorithms in terms of their error,
output data size, parameter combinations, etc. allowing them to have
the best information on choosing a set of data reduction algorithms
available.</p>
<p>Soumya Dutta (LANL, OSU), Tzu-Hsuan Wei (LANL, OSU), Max Zeyen 
(LANL, TU Kaiserslautern), Emily Casleton (LANL), Joanne Wendelberger (LANL),
Jon Woodring (LANL)</p>
</p><a href="#Scoreboard:-Designing-a-Platform-for-Comparing-Triage-Algorithms"><small> 
Link to this post</small></a><hr><br><h3>FY15 Progress and Updates</h3><a name="FY15-Progress-and-Updates" href="#FY15-Progress-and-Updates" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>See below for several updates on
the FY15 progress and results of the EDDA project.</p>
<p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data">Direct link to the first FY15 update</a></p>
<p><a href="pdf/fy15-osu-public.pdf">Download the OSU FY15 progress report</a></p>
</p><a href="#FY15-Progress-and-Updates"><small> 
Link to this post</small></a><hr><br><h3>A Novel Approach for Approximate Aggregations over Arrays</h3><a name="A-Novel-Approach-for-Approximate-Aggregations-over-Arrays" href="#A-Novel-Approach-for-Approximate-Aggregations-over-Arrays" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/aggregate-workflow.png" alt="approximate aggregation workflow"></p>
<p>Approximate aggregation has been a popular approach for interactive data analysis and decision making, especially on large-scale datasets. While there is clearly a need to apply this approach for scientific datasets comprising massive arrays, existing algorithms have largely been developed for relational data, and cannot handle both dimension-based and value-based predicates efficiently while maintaining accuracy. We present a novel approach for approximate aggregations over array data, using bitmap indices or bitvectors as the summary structure, as they preserve both spatial and value distribution of the data. We develop approximate aggregation algorithms using only the bitvectors and certain additional pre-aggregation statistics (equivalent to a 1-dimensional histogram) that we require. Another key development is choosing a binning strategy that can improve aggregation accuracy -- we introduce a v-optimized binning strategy and its weighted extension, and present a bitmap construction algorithm with such binning. We compare our method with other existing methods including sampling and multi-dimensional histograms, as well as the use of other binning strategies with bitmaps. We demonstrate both high accuracy and efficiency of our approach. Specifically, we show that in most cases, our method is more accurate than other methods by at least one order of magnitude. Despite achieving much higher accuracy, our method can require significantly less storage than multi-dimensional histograms.</p>
<p><a href="http://dl.acm.org/citation.cfm?doid=2791347.2791349">Link to paper</a> |
<a href="http://www.cse.ohio-state.edu/~agrawal/Slides/SSDBM2015_yw.pptx">Link to slides</a></p>
</p><a href="#A-Novel-Approach-for-Approximate-Aggregations-over-Arrays"><small> 
Link to this post</small></a><hr><br><h3>In-Situ Bitmaps Generation and Efficient Data Analysis based on Bitmaps</h3><a name="In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps" href="#In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/bitmap-mining.png" alt="bitmap correlation mining"></p>
<p>Neither the memory capacity, memory access speeds, nor disk bandwidths are increasing at the same rate as the computing power in current and upcoming parallel machines. This has led to considerable recent research on in-situ data analytics. However, many open questions remain on how to perform such analytics, especially in memory constrained systems. Building on our earlier work that demonstrated bitmap indices (bitmaps) can be a suitable summary structure for key (offline) analytics tasks,  we have developed an in-situ analysis approach that performs data reduction (such as time-steps selection) using just bitmaps, and subsequently, stores only the selected bitmaps for post-analysis. We construct compressed bitmaps on the fly, show that many kinds of in-situ analyses can be supported by bitmaps without requiring the original data (and thus reducing memory requirements for in-situ analysis), and instead of writing the original simulation output, we only write the selected bitmaps to the disks (reducing the I/O requirements). We also demonstrate that we are able to use bitmaps for key offline analysis steps. We extensively evaluate our method with different simulations and applications, and demonstrate the effectiveness of our approach.</p>
<p><a href="http://dl.acm.org/citation.cfm?doid=2749246.2749268">Link to paper</a> |
<a href="http://www.cse.ohio-state.edu/~agrawal/Slides/YuSuHPDC2015.pptx">Link to slides</a></p>
</p><a href="#In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps"><small> 
Link to this post</small></a><hr><br><h3>Towards Statistically-based Error Metrics for Computationally-driven Data Triage</h3><a name="Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/size-time-tradeoff.png" alt="size vs. accuracy"></p>
<p>Computing power has increased faster than the amount of storage bandwidth or the ability to read stored data. Thus, researchers can perform large-scale, high-resolution simulations, but they are unable to load all the data into local memory. Performing queries becomes necessary; however, as the size of the data increases, queries will also be time-consuming.  Bitmap indexing was originally developed for faster query processing of read-mostly data, but has recently been used for scientific data management. It consists of a set of bit vectors, where one vector corresponds to a distinct attribute value or range of values. Each bit is mapped to a record, and the bit value is 1 if the record matches the property in focus.  Bitmap indexing is able to perform complex logical operations quickly, but for floating-point attributes, the bitmap indexing will be lossy because the bit vector values must be binned.  Different strategies have been proposed to bin bit vectors value.  </p>
<p>A review of the current literature on bitmap indexing has identified the need for a comparative error metric to assess the binning scheme of bit vectors.  This metric will incorporate multiple, conflicting criteria, such as query processing speed and index size, provide uncertainty on the assessment, and allow for a sensitivity analysis. This error metric will eventually be used as the bitmap index is being created in situ to indicate when the current binning strategy is no longer optimal.</p>
<p>Emily Casleton (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>), Jon Woodring (<em>LANL</em>)</p>
</p><a href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage"><small> 
Link to this post</small></a><hr><br><h3>A Generalized Framework for Comparing across Data Representations</h3><a name="A-Generalized-Framework-for-Comparing-across-Data-Representations" href="#A-Generalized-Framework-for-Comparing-across-Data-Representations" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/scoreboard.png" alt="scoreboard"></p>
<p>In the era of big data analytics, efficient data transformation and summarization is becoming a popular approach for big data handling. Since it is almost impossible to store all the raw data, an information rich data representation can benefit data analysts immensely by reducing the size of the data to a manageable scale yet preserving the features in the data with high accuracy. In order to perform this task, we need to identify data summarization techniques which are compact, easy to compute and represent. Furthermore, a comparative framework is also need to be devised which will take different data summarization techniques and compare their effectiveness in terms of several critical parameters such as representation accuracy, storage cost, computation time, errors incurred etc. This work aims at producing such a comparative framework. Majority of the data summarization techniques try to prioritize the data by partitioning it into smaller regions and summarizing each region with appropriate representatives. Each of such representation algorithms in general can be divided into three steps: (1) data partitioning, (2) partition summarization and (3) error estimation. Therefore the accuracy and effectiveness of those representative algorithms is of prime importance to data scientists. To address these requirements, we have created a score-boarding framework which accepts several data summarization techniques and performs a global scale parameter study on them. Such a parameter study scheme is able to identify data specific transformation and summarization techniques which yields the best representation for the data. Since these transformation and summarization methods can not capture all the information the data has, appropriate error metrics are also an importance parameter in this study. By performing a comprehensive parameter study on a data set, we can analyze the results and find the best scheme of data summarization and the associated combination of parameters for it. The output of our framework is a database table where we keep track of all the test combinations and the results obtained by them in terms of data reduction, effectiveness of the summarization scheme, time taken to compute etc. By querying the output database, we can easily find the most effective algorithm and the associated parameter combinations which obtained the best result. </p>
<p>Soumya Dutta (<em>LANL</em>, <em>OSU</em>), Emily Casleton (<em>LANL</em>), Ayan Biswas (<em>LANL</em>, <em>OSU</em>), Jon Woodring (<em>LANL</em>), Jim Ahrens (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>)</p>
</p><a href="#A-Generalized-Framework-for-Comparing-across-Data-Representations"><small> 
Link to this post</small></a><hr><br><h3>Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis</h3><a name="Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/gmm-tracking.png" alt="GMM-based tracking"></p>
<p>Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7192664">Link to paper</a></p>
</p><a href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis"><small> 
Link to this post</small></a><hr><br><h3>Efficient Local Histogram Searching via Bitmap Indexing</h3><a name="Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-searching.png" alt="local histogram search"></p>
<p>Representing features by local histograms is a proven technique in several volume analysis and visualization applications including feature tracking and transfer function design. The efficiency of these applications, however, is hampered by the high computational complexity of local histogram computation and matching. In this paper, we propose a novel algorithm to accelerate local histogram search by leveraging bitmap indexing. Our method avoids exhaustive searching of all voxels in the spatial domain by examining only the voxels whose values fall within the value range of user-defined local features and their neighborhood. Based on the idea that the value range of local features is in general much smaller than the dynamic range of the entire dataset, we propose a local voting scheme to construct the local histograms so that only a small number of voxels need to be examined. Experimental results show that our method can reduce much computational workload compared to the conventional approaches. To demonstrate the utility of our method, an interactive interface was developed to assist users in defining target features as local histograms and identify the locations of these features in the dataset.</p>
<p><a href="http://onlinelibrary.wiley.com/doi/10.1111/cgf.12620/abstract">Link to paper</a></p>
</p><a href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing"><small> 
Link to this post</small></a><hr><br><h3>A Compact Multivariate Histogram Representation for Query-driven Visualization</h3><a name="A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-query.png" alt="pipeline for histogram query"></p>
<p>As the size of data continues to increase, distribution-based methods become increasingly more important for data summarization and queries. To represent the distribution from a dataset without relying on a particular parametric model, histograms are widely used in many applications as it is simple to create and efficient to query. For multivariate scientific datasets, however, storing multivariate histograms in the form of multi-dimensional arrays is very expensive as the size of the histogram grows exponentially with the number of variables. In this paper, we present a compact structure to store multivariate histograms to reduce its huge space cost while supporting different kinds of histogram queries efficiently. A data space transformation is employed first to transform the large multi-dimensional array to a much smaller array. Dictionaries are constructed to encode this transformation. Then, the multivariate histogram is represented as a sequence of index and frequency pairs where the indices are represented as bitstrings computed from a space filling curve traversal of the transformed array. With this compact representation, the storage cost for the histograms is reduced. Based on our representation, we also present several common types of queries such as histogram marginalization, bin-merging and computation of conditional probability. We parallelize both the histogram computation and queries to improve its efficiency. We present several query-driven visualization applications to explore and analyze multivariate scientific datasets. Experimental results to study the performance of our framework in terms of scalability and space cost are also discussed.</p>
<p>Kewei Lu, Han-Wei Shen. <em>The 5th IEEE Symposium on Large Data Analysis and Visualization</em>, 2015.</p>
</p><a href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization"><small> 
Link to this post</small></a><hr><br><h3>An Uncertainty-Driven Approach to Vortex Analysis Using Oracle Consensus and Spatial Proximity</h3><a name="An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/vortex-analysis.png" alt="vortex detection"></p>
<p>Although vortex analysis and detection have been extensively investigated in the past, none of the existing techniques are able to provide fully robust and reliable identification results. Local vortex detection methods are popular as they are efficient and easy to implement, and produce binary outputs based on a user-specified, hard threshold. However, vortices are global features, which present challenges for local detectors. On the other hand, global detectors are computationally intensive and require considerable user input. In this work, we propose a consensus-based uncertainty model and introduce spatial proximity to enhance vortex detection results obtained using point-based methods. We use four existing local vortex detectors and convert their outputs into fuzzy possibility values using a sigmoid-based soft-thresholding approach. We apply a majority voting scheme that enables us to identify candidate vortex regions with a higher degree of confidence. Then, we introduce spatial proximity- based analysis to discern the final vortical regions. Thus, by using spatial proximity coupled with fuzzy inputs, we propose a novel uncertainty analysis approach for vortex detection. We use expert’s input to better estimate the system parameters and results from two real-world data sets demonstrate the efficacy of our method.</p>
<p><a href="http://web.cse.ohio-state.edu/~biswas/template_2.pdf">Link to paper</a></p>
</p><a href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity"><small> 
Link to this post</small></a><hr><br><h3>Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation</h3><a name="Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/rotating-stall.png" alt="detection of stall"></p>
<p>Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data inmore detail. To achieve this, we use a statistical anomaly detection method to locate possible stall inception.  We further derive algorithms from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.</p>
<p><a href="http://dx.doi.org/10.1109/TVCG.2015.2467952">Link to paper</a></p>
</p><a href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation"><small> 
Link to this post</small></a><hr><br><h3>Software for Distribution Data Modeling and Visualization</h3><a name="Software-for-Distribution-Data-Modeling-and-Visualization" href="#Software-for-Distribution-Data-Modeling-and-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The <a href="https://sites.google.com/site/gravityvisdb/edda">EDDA library</a> aims at visualizing distribution data for uncertainty analysis.  </p>
<p>The goal is to provide a unified data model with generic distribution representations for the development of uncertainty visualization algorithms.  The distribution models to support will be parametric distributions like Gaussian and GMM, un-parametric distributions like histogram and KDE, as well as joint distributions.  These are encapsulated into C++ template classes.  Coupled with our experiences on developing regular and curvilinear-grid datasets in OSUFlow, we provide an API allowing to query for the distribution of a given 3D position.  The return of the query can be either the interpolated distribution or a Monte-Carlo sample of the distribution, depending on the need of the visualization algorithm.  We also provide distribution arithmetic and analysis tools including project ITL.</p>
<p>Chun-Ming Chen, <em>et al.</em> (<em>OSU</em>)</p>
</p><a href="#Software-for-Distribution-Data-Modeling-and-Visualization"><small> 
Link to this post</small></a><hr><br><h3>Uncertainty Modeling and Error Reduction for Pathline Computation in Time-varying Flow Fields</h3><a name="Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/uncertainty-modeling-1.png" alt="uncertain pathline"> <img src="images/uncertainty-modeling-2.png" alt="uncertain-pathline"></p>
<p>When the spatial and temporal resolutions of a time-varying simulation become very high, it is not possible to process or store data from every time step due to the high computation and storage cost. Although using uniformly down-sampled data for visualization is a common practice, important information in the un-stored data can be lost. Currently, linear interpolation is a popular method used to approximate data between the stored time steps. For pathline computation, however, errors from the interpolated velocity in the time dimension can accumulate quickly and make the trajectories rather unreliable. To inform the scientist the error involved in the visualization, it is important to quantify and display the uncertainty, and more importantly, to reduce the error whenever possible. In this paper, we present an algorithm to model temporal interpolation error, and an error reduction scheme to improve the data accuracy for temporally down-sampled data. We show that it is possible to compute polynomial regression and measure the interpolation errors incrementally with one sequential scan of the time-varying flow field. We also show empirically that when the data sequence is fitted with least-squares regression, the errors can be approximated with a Gaussian distribution. With the end positions of particle traces stored, we show that our error modeling scheme can better estimate the intermediate particle trajectories between the stored time steps based on a maximum likelihood method that utilizes forward and backward particle traces.</p>
<p><a href="http://dx.doi.org/10.1109/PACIFICVIS.2015.715638">Link to paper</a></p>
</p><a href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields"><small> 
Link to this post</small></a><hr><br><h3>Association Analysis for Visual Exploration of Multivariate Scientific Data Sets</h3><a name="Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/association-analysis.png" alt="association analysis"></p>
<p>The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data’s multi-faceted properties. We present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7192697">Link to paper</a></p>
</p><a href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Self-Adaptive Density Estimation of Particle Data</h3><a name="Self-Adaptive-Density-Estimation-of-Particle-Data" href="#Self-Adaptive-Density-Estimation-of-Particle-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/density-estimate.png" alt="cosmology particles and density field"></p>
<p>We conducted a study of density estimation, the conversion of discrete particle positions to a continuous field of particle density defined over a 3D Cartesian grid. The study features a methodology for evaluating the accuracy and performance of various density estimation methods, results of that evaluation for four density estimators, and a large-scale parallel algorithm for a self-adaptive method that computes a Voronoi tessellation as an intermediate step. We demonstrated the performance and scalability of our parallel algorithm on a supercomputer when estimating the density of 100 million particles over 500 billion grid points.</p>
<p>Peterka, <em>et al.</em> (<em>ANL</em>) Submitted to <em>SIAM Journal on Scientific Computing SISC Special Section on CSE15: Software and Big Data</em>, 2015.</p>
</p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data"><small> 
Link to this post</small></a><hr><br><h3>SDMAV Kick-Off PI Meeting</h3><a name="SDMAV-Kick-Off-PI-Meeting" href="#SDMAV-Kick-Off-PI-Meeting" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On Tuesday, January 13-15, there will be a Principal Investigator (PI)
Meeting, for projects funded under the Scientific Data Management,
Analysis and Visualization (SDMAV) by DOE SC ASCR, in Walnut Creek, CA.
EDDA PIs and investigators will be present for this meeting. Below
are links to download materials to be presented at this meeting for the
EDDA project.</p>
<ul>
<li><a href="files/2015-1-6/EDDA-Jan2015-Handout.docx">Handout</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Quad.ppt">Quad Chart</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Poster.pdf">Poster</a></li>
</ul>
</p><a href="#SDMAV-Kick-Off-PI-Meeting"><small> 
Link to this post</small></a><hr><br><h3>Logo Explanation</h3><a name="Logo-Explanation" href="#Logo-Explanation" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The acronym for our project is EDDA, which if you google for it you find
the description for Edda on <a href="http://en.wikipedia.org/wiki/Edda">wikipedia</a>.</p>
<blockquote>
<p>The term &quot;Edda&quot; (/ˈɛdə/; Old Norse Edda, plural Eddur) applies to 
the Old Norse Poetic Edda and Prose Edda, both of which were written 
down in Iceland during the 13th century in Icelandic, although they 
contain material from earlier traditional sources, reaching into the 
Viking Age. The books are the main sources of medieval skaldic tradition 
in Iceland and Norse mythology. </p>
</blockquote>
<p>Runic alphabets, in particular <em>futhark</em> (&quot;th&quot; is the thorn: þ, fuþark), 
was used by Scandanavian (Norse) and its use was noted use in Eddic lore.</p>
<blockquote>
<p>In Norse mythology, the runic alphabet is attested to a divine origin 
(Old Norse: reginkunnr). This is attested as early as on the Noleby 
Runestone from approximately 600 AD that reads Runo fahi raginakundo 
toj[e&#39;k]a..., meaning &quot;I prepare the suitable divine rune...&quot; and in 
an attestation from the 9th century on the Sparlösa Runestone, which 
reads Ok rað runaR þaR rægi[n]kundu, meaning &quot;And interpret the runes 
of divine origin&quot;.</p>
</blockquote>
<p>J.R.R. Tolkien used derivatives of <em>futhark</em> to describe the alphabet
used by the dwarves and even created his own called <em>Cirth</em>.</p>
<p>Well, what does this all mean for our logo? It&#39;s the transliterated
version of EDDA into <em>elder futhark</em>.</p>
<ul>
<li>E = ehwaz = &quot;M&quot; like character</li>
<li>D = dagaz = &quot;infinity&quot; like character</li>
<li>A = ansuz = &quot;F&quot; like character</li>
</ul>
</p><a href="#Logo-Explanation"><small> 
Link to this post</small></a><hr><br><h3>Introduction</h3><a name="Introduction" href="#Introduction" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>As it becomes more difficult to analyze large-scale simulation output at full
resolution, users will have to review and identify regions of interest by
transforming data into compact information descriptors that characterize
simulation results and allow detailed analysis on demand. This is because
exascale architectures will be much more constrained with respect to data
movement, and <em>in situ</em> data processing will be the norm, where the goals are to
fit the total amount of output data within a budget, to summarize and triage
data based on content, and to classify and index data to facilitate efficient
offline analysis. In addition, <em>in situ</em> analysis must be performed in a time 
and space efficient fashion, not only to avoid slowing down the simulation, but
also to not consume too much memory.  </p>
<p>Among many different feature descriptors,
the statistical information derived from data samples is a promising approach
to taming the big data avalanche, because data distributions computed from a
population can compactly describe the presence and characteristics of salient
features with minimal data movement.  The ability to computationally summarize
and process data using distributions provides an efficient and representative
capture of the information content of a large-scale data set. This
representation can adjust to size and resource constraints, with the added
benefit that uncertainty can be quantified and communicated.  </p>
<p>In this project,
<em>we posit that with the growing number of cores per node, with increasing memory
and I/O constraints in emerging extreme-scale platforms, it will be feasible
and desirable to compute distributions at simulation time, perform
memory-efficient</em> in situ <em>analysis using distributions, and save distributions
as a space-efficient summarization for on-demand, offline visualization and
analysis of salient features.</em> The key development will be a novel
distribution-based analysis and visualization framework based on in situ pro-
cessing of extreme-scale scientific data. Our goals are to ensure that
scientists can easily obtain an overview of the entire data set regardless of
the size of the simulation; understand the characteristics and locations of the
features; easily interact with the data and select regions and features of
interest; and perform all the analysis tasks with a small memory footprint.</p>
</p><a href="#Introduction"><small> 
Link to this post</small></a><hr><br></p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small><script src="javascripts/scale.fix.js"></script></body></html>