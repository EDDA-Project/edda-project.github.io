<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>EDDA: Extreme-scale Distribution-based Data Analysis</title><link rel="stylesheet" href="stylesheets/styles.css"><link rel="stylesheet" href="stylesheets/pygment_trac.css"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><!--if lt IE 9script(src='//html5shiv.googlecode.com/svn/trunk/html5.js')--></head><body><a href="#Logo-Explanation"><img src="images/edda.png" width="25%"></a><h1> <a href="https://github.com/EDDA-Project">EDDA: Extreme-scale Distribution-based Data Analysis</a></h1><h3>The Ohio State University, Argonne National Laboratory,
Los Alamos National Laboratory<br>Han-Wei Shen (OSU), Tom Peterka (ANL), Jon Woodring (LANL)<br>Gagan Agrawal (OSU), Huamin Wang (OSU), Joanne Wendelberger (LANL)</h3><h3><a href="http://science.energy.gov/ascr/">DOE SC ASCR </a>project on the <a href="#Introduction">research for using distributions
as a proxy for large-scale data analysis.</a></h3><small>LA-UR-15-20108</small><hr><p><h3>Uncertainty Modeling and Error Reduction for Pathline Computation in Time-varying Flow Fields</h3><a name="Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/uncertainty-modeling-1.png" alt="uncertain pathline"> <img src="images/uncertainty-modeling-2.png" alt="uncertain-pathline"></p>
<p>When the spatial and temporal resolutions of a time-varying simulation become very high, it is not possible to process or store data from every time step due to the high computation and storage cost. Although using uniformly down-sampled data for visualization is a common practice, important information in the un-stored data can be lost. Currently, linear interpolation is a popular method used to approximate data between the stored time steps. For pathline computation, however, errors from the interpolated velocity in the time dimension can accumulate quickly and make the trajectories rather unreliable. To inform the scientist the error involved in the visualization, it is important to quantify and display the uncertainty, and more importantly, to reduce the error whenever possible. In this paper, we present an algorithm to model temporal interpolation error, and an error reduction scheme to improve the data accuracy for temporally down-sampled data. We show that it is possible to compute polynomial regression and measure the interpolation errors incrementally with one sequential scan of the time-varying flow field. We also show empirically that when the data sequence is fitted with least-squares regression, the errors can be approximated with a Gaussian distribution. With the end positions of particle traces stored, we show that our error modeling scheme can better estimate the intermediate particle trajectories between the stored time steps based on a maximum likelihood method that utilizes forward and backward particle traces.</p>
<p><a href="http://dx.doi.org/10.1109/PACIFICVIS.2015.715638">Link to publication</a></p>
</p><a href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields"><small> 
Link to this post</small></a><hr><br><h3>Association Analysis for Visual Exploration of Multivariate Scientific Data Sets</h3><a name="Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/association-analysis.png" alt="association analysis"></p>
<p>The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data’s multi-faceted properties. We present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.</p>
<p>Xiaotong Liu and Han-Wei Shen. <em>IEEE Transactions on Visualization and Computer Graphics (TVCG)</em>, 2015.</p>
</p><a href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Self-Adaptive Density Estimation of Particle Data</h3><a name="Self-Adaptive-Density-Estimation-of-Particle-Data" href="#Self-Adaptive-Density-Estimation-of-Particle-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/density-estimate.png" alt="cosmology particles and density field"></p>
<p>We conducted a study of density estimation, the conversion of discrete particle positions to a continuous field of particle density defined over a 3D Cartesian grid. The study features a methodology for evaluating the accuracy and performance of various density estimation methods, results of that evaluation for four density estimators, and a large-scale parallel algorithm for a self-adaptive method that computes a Voronoi tessellation as an intermediate step. We demonstrated the performance and scalability of our parallel algorithm on a supercomputer when estimating the density of 100 million particles over 500 billion grid points.</p>
<p><em>Submitted to SIAM Journal on Scientific Computing SISC Special Section on CSE15: Software and Big Data, 2015.</em></p>
</p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data"><small> 
Link to this post</small></a><hr><br><h3>SDMAV Kick-Off PI Meeting</h3><a name="SDMAV-Kick-Off-PI-Meeting" href="#SDMAV-Kick-Off-PI-Meeting" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On Tuesday, January 13-15, there will be a Principal Investigator (PI)
Meeting, for projects funded under the Scientific Data Management,
Analysis and Visualization (SDMAV) by DOE SC ASCR, in Walnut Creek, CA.
EDDA PIs and investigators will be present for this meeting. Below
are links to download materials to be presented at this meeting for the
EDDA project.</p>
<ul>
<li><a href="files/2015-1-6/EDDA-Jan2015-Handout.docx">Handout</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Quad.ppt">Quad Chart</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Poster.pdf">Poster</a></li>
</ul>
</p><a href="#SDMAV-Kick-Off-PI-Meeting"><small> 
Link to this post</small></a><hr><br><h3>Logo Explanation</h3><a name="Logo-Explanation" href="#Logo-Explanation" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The acronym for our project is EDDA, which if you google for it you find
the description for Edda on <a href="http://en.wikipedia.org/wiki/Edda">wikipedia</a>.</p>
<blockquote>
<p>The term &quot;Edda&quot; (/ˈɛdə/; Old Norse Edda, plural Eddur) applies to 
the Old Norse Poetic Edda and Prose Edda, both of which were written 
down in Iceland during the 13th century in Icelandic, although they 
contain material from earlier traditional sources, reaching into the 
Viking Age. The books are the main sources of medieval skaldic tradition 
in Iceland and Norse mythology. </p>
</blockquote>
<p>Runic alphabets, in particular <em>futhark</em> (&quot;th&quot; is the thorn: þ, fuþark), 
was used by Scandanavian (Norse) and its use was noted use in Eddic lore.</p>
<blockquote>
<p>In Norse mythology, the runic alphabet is attested to a divine origin 
(Old Norse: reginkunnr). This is attested as early as on the Noleby 
Runestone from approximately 600 AD that reads Runo fahi raginakundo 
toj[e&#39;k]a..., meaning &quot;I prepare the suitable divine rune...&quot; and in 
an attestation from the 9th century on the Sparlösa Runestone, which 
reads Ok rað runaR þaR rægi[n]kundu, meaning &quot;And interpret the runes 
of divine origin&quot;.</p>
</blockquote>
<p>J.R.R. Tolkien used derivatives of <em>futhark</em> to describe the alphabet
used by the dwarves and even created his own called <em>Cirth</em>.</p>
<p>Well, what does this all mean for our logo? It&#39;s the transliterated
version of EDDA into <em>elder futhark</em>.</p>
<ul>
<li>E = ehwaz = &quot;M&quot; like character</li>
<li>D = dagaz = &quot;infinity&quot; like character</li>
<li>A = ansuz = &quot;F&quot; like character</li>
</ul>
</p><a href="#Logo-Explanation"><small> 
Link to this post</small></a><hr><br><h3>Introduction</h3><a name="Introduction" href="#Introduction" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>As it becomes more difficult to analyze large-scale simulation output at full
resolution, users will have to review and identify regions of interest by
transforming data into compact information descriptors that characterize
simulation results and allow detailed analysis on demand. This is because
exascale architectures will be much more constrained with respect to data
movement, and <em>in situ</em> data processing will be the norm, where the goals are to
fit the total amount of output data within a budget, to summarize and triage
data based on content, and to classify and index data to facilitate efficient
offline analysis. In addition, <em>in situ</em> analysis must be performed in a time 
and space efficient fashion, not only to avoid slowing down the simulation, but
also to not consume too much memory.  </p>
<p>Among many different feature descriptors,
the statistical information derived from data samples is a promising approach
to taming the big data avalanche, because data distributions computed from a
population can compactly describe the presence and characteristics of salient
features with minimal data movement.  The ability to computationally summarize
and process data using distributions provides an efficient and representative
capture of the information content of a large-scale data set. This
representation can adjust to size and resource constraints, with the added
benefit that uncertainty can be quantified and communicated.  </p>
<p>In this project,
<em>we posit that with the growing number of cores per node, with increasing memory
and I/O constraints in emerging extreme-scale platforms, it will be feasible
and desirable to compute distributions at simulation time, perform
memory-efficient</em> in situ <em>analysis using distributions, and save distributions
as a space-efficient summarization for on-demand, offline visualization and
analysis of salient features.</em> The key development will be a novel
distribution-based analysis and visualization framework based on in situ pro-
cessing of extreme-scale scientific data. Our goals are to ensure that
scientists can easily obtain an overview of the entire data set regardless of
the size of the simulation; understand the characteristics and locations of the
features; easily interact with the data and select regions and features of
interest; and perform all the analysis tasks with a small memory footprint.</p>
</p><a href="#Introduction"><small> 
Link to this post</small></a><hr><br></p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small><script src="javascripts/scale.fix.js"></script></body></html>