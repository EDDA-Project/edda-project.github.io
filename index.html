<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="chrome=1"><title>EDDA: Extreme-scale Distribution-based Data Analysis</title><link rel="stylesheet" href="stylesheets/styles.css"><link rel="stylesheet" href="stylesheets/pygment_trac.css"><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><!--if lt IE 9script(src='//html5shiv.googlecode.com/svn/trunk/html5.js')--></head><body><a href="#Logo-Explanation"><img src="images/edda.png" width="25%"></a><h1> <a href="https://github.com/EDDA-Project">EDDA: Extreme-scale Distribution-based Data Analysis</a></h1><h3>The Ohio State University, Argonne National Laboratory,
Los Alamos National Laboratory<br>Han-Wei Shen (OSU), Tom Peterka (ANL), Jon Woodring (LANL)<br>Gagan Agrawal (OSU), Huamin Wang (OSU), Joanne Wendelberger (LANL)</h3><h3><a href="http://science.energy.gov/ascr/">DOE SC ASCR </a>project on the <a href="#Introduction">research for using distributions
as a proxy for large-scale data analysis.</a></h3><small>LA-UR-15-20108</small><hr><p><h3>FY16 &amp; first half of FY17 Progress and Updates</h3><a name="FY16-&amp;-first-half-of-FY17-Progress-and-Updates" href="#FY16-&amp;-first-half-of-FY17-Progress-and-Updates" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On March 14-16, we will be having a project PI meeting 
at the Hyatt in Bethesda, MD. Below are links to the
handout, poster, and quad chart presenting the update
material on the EDDA project that will be presented at
the meeting.</p>
<p><a href="files/2017-3-13/handout_ascr_osu.docx">Handout</a> |
<a href="files/2017-3-13/poster_ascr_osu.ppt">Poster</a> |
<a href="files/2017-3-13/quad_ascr_osu.ppt">Quad Chart</a></p>
<p>Detailed updates are contained in the following posts,
with a link to the first in the set.</p>
<p><a href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets">Direct link to the first</a></p>
</p><a href="#FY16-&amp;-first-half-of-FY17-Progress-and-Updates"><small> 
Link to this post</small></a><hr><br><h3>Multi-Resolution Climate Ensemble Parameter Analysis with Nested Parallel Coordinates Plot (NPCP)</h3><a name="Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)" href="#Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/npcp1.png" alt="Nested Parallel Coordinate Plot"></p>
<p><img src="images/2017-3-13/npcp2.png" alt="Nested Parallel Coordinate Plot"></p>
<p><strong>Problem</strong>: This work studies the input parameters (multi-resolution
convective parameters) of multi-resolution climate simulations. Domain experts
are specifically interested in: the correlations between parameters in the same
resolution (intra-resolution correlation visualization) and the difference
between parameters’ correlations in different resolutions (inter-resolution
correlation comparison). It is also critical to build the connection between
the multi-resolution convective parameters and the large spatial-temporal
climate ensemble outputs.</p>
<p><strong>Proposed Solution</strong>: Based on the requirements from domain experts, we
propose an augmented design of parallel coordinates plots, called Nested
Parallel Coordinates Plot (NPCP), to visualize the multi-resolution convective
parameters in climate modeling. This new design of PCP has been integrated into
a visual analytics system, which is equipped with multiple coordinated views,
to help domain scientists build the connection between complex input parameter
settings and spatial temporal ensemble outputs.</p>
<p><a href="files/2017-3-13/climate_npcp.pptx">Link to the slide</a></p>
</p><a href="#Multi-Resolution-Climate-Ensemble-Parameter-Analysis-with-Nested-Parallel-Coordinates-Plot-(NPCP)"><small> 
Link to this post</small></a><hr><br><h3>Virtual Retractor: An Interactive Data Exploration System Using Physically Based Deformation</h3><a name="Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation" href="#Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/retractor1.png" alt="Virtual Retractor"></p>
<p><img src="images/2017-3-13/retractor2.png" alt="Virtual Retractor"></p>
<p><strong>Problem</strong>: Interactive data exploration plays a fundamental role in analyzing
three dimensional scientific data. Occlusion management and feature
preservation are among the key factors to ensure effective identification and
extraction of three-dimensional features. Existing methods may not able to
combine the occlusion removal task, together with preserving features flexibly
defined by data properties, while providing interactions with real-time
performance.</p>
<p><strong>Proposed Solution</strong>: we propose a new data exploration system that allows
direct manipulation of data with cutting and splitting capabilities. The
procedure is carried out by deforming a tetrahedral mesh that has a void in the
middle to simulate the incision created by the cut. The splitting operation
enlarges the void to allow users to observe the inner structure which was
originally occluded. Our mesh is constructed with the local data properties
taken into account, such as local data density or gradient. Therefore the
deformation will affected by the selected data property. Regions with high data
property values are more solid and harder to be deformed, while regions with
lower property values will be deformed more. Therefore the deformation can keep
interesting features of the data.</p>
<p><a href="files/2017-3-13/virtual_retractor.pptx">Link to the slide</a></p>
</p><a href="#Virtual-Retractor:-An-Interactive-Data-Exploration-System-Using-Physically-Based-Deformation"><small> 
Link to this post</small></a><hr><br><h3>Range Likelihood Tree: A Compact and Effective Representation for Visual Exploration of Uncertain Data Sets</h3><a name="Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets" href="#Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/range_tree1.png" alt="Range Likelihood Tree"></p>
<p><img src="images/2017-3-13/range_tree2.png" alt="Range Likelihood Tree"></p>
<p><img src="images/2017-3-13/range_tree3.png" alt="Range Likelihood Tree"></p>
<p><strong>Problem</strong>: Uncertain data visualization plays a fundamental role in many
applications such as weather forecast and analysis of fluid flows. Exploring
scalar uncertain data modeled as probability distribution fields is a
challenging task because the underlying features are often more complex, and
the data associated with each grid point are high dimensional.</p>
<p><strong>Proposed Solution</strong>: In this work, we present a compact and effective
representation called Range Likelihood Tree (RLT), to summarize and explore
probability distribution fields. The key idea is to consider the different
roles that subranges (subspaces of the value domain) may play in understanding
probability distributions, and decompose and summarize each complex probability
distribution over a few representative subranges by cumulative probabilities.
In our method, the value domain is first partitioned into subranges, then the
distribution at each grid point is transformed according to the cumulative
probabilities of the point’s distribution in those subranges. Organizing the
subranges into a hierarchical structure based on how these cumulative
probabilities are spatially distributed in the grid points, the new range
likelihood tree representation allows effective classification and
identification of features through user query and exploration. We present an
exploration framework with multiple interactive views to explore probability
distribution fields, and provide guidelines for visual exploration using our
framework.</p>
<p><a href="files/2017-3-13/range_tree.pptx">Link to the slide</a></p>
</p><a href="#Range-Likelihood-Tree:-A-Compact-and-Effective-Representation-for-Visual-Exploration-of-Uncertain-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Homogeneity Guided Probabilistic Data Summaries for Analysis and Visualization of Large-Scale Data Sets</h3><a name="Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets" href="#Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/hguided1.png" alt="Homogeneity Guided Probabilistic Data Summaries"></p>
<p><img src="images/2017-3-13/hguided2.png" alt="Homogeneity Guided Probabilistic Data Summaries"></p>
<p><strong>Problem</strong>: </p>
<ul>
<li>Existing region-based statistical data summaries rely upon regular
partitioning  that results in partitions with high data value variation and
uncertainty leading to increased sampling error is analysis </li>
<li>Visualizations produced from these statistical data summarizations introduce
artifacts and distortions making visual analysis less effective</li>
</ul>
<p><strong>Solution</strong>: </p>
<ul>
<li>We propose a homogeneous region guided data partitioning which employs a fast
clustering algorithm SLIC for in situ partition generation </li>
<li>SLIC based homogeneous partitions are summarized using a hybrid distribution
scheme of either a single Gaussian or mixture of Gaussians per partition</li>
<li>Extensive quantitative and qualitative comparisons of our method with regular
partitioning and k-d tree based partitioning reveal that our method is
superior in quality vs storage trade-off and can be performed in situ in a
scalable way </li>
</ul>
<p><a href="files/2017-3-13/homogenity_guided.pptx">Link to the slide</a></p>
</p><a href="#Homogeneity-Guided-Probabilistic-Data-Summaries-for-Analysis-and-Visualization-of-Large-Scale-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Statistical Visualization and Analysis of Large Data Using a Value-based Spatial Distribution</h3><a name="Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution" href="#Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/value_space1.png" alt="Value-based Spatial Distribution"></p>
<p><img src="images/2017-3-13/value_space2.png" alt="Value-based Spatial Distribution"></p>
<p><strong>Problem</strong>: The computational power of modern supercomputers allow scientists
to model physical phenomena with high-resolution simulation. However, analyzing
such large-scale scientific simulation data is challenging due to the
incompatibility between memory limitations, I/O capacities, and high
computational power. Using distribution-based representation to handle big data
sets becomes popular, but the distribution inherently lacks the spatial
information of samples and causes the low visualization quality. Developing the
technique to improve visualization quality from the distribution based
representation is necessary.</p>
<p><strong>Proposed Solution</strong>: In addition to the traditional value distribution, we
construct and store the spatial distributions where the locations of samples
are collected and stored as a multi-dimensional distribution for each value
sub-range. Each multi-dimensional distribution is stored using compact
distribution representation which is Spatial Gaussian Mixture Model (GMM). The
Spatial GMM maps the locations of the data points in different value ranges to
probabilities. When visualizing the data set, we utilize our representation to
infer the probability for a value to reside at arbitrary location using Bayes’
rule, which combines known information (the value distribution) and  additional
evidences (the Spatial GMMs) from a given condition. Equipped with this spatial
information, our approach produces lower variance, and hence lower uncertainty,
in the results of statistical based analysis and visualizations.</p>
<p><a href="files/2017-3-13/spatial_distribution.pptx">Link to the slide</a></p>
</p><a href="#Statistical-Visualization-and-Analysis-of-Large-Data-Using-a-Value-based-Spatial-Distribution"><small> 
Link to this post</small></a><hr><br><h3>Visualizing the Variations of Ensemble Isosurfaces</h3><a name="Visualizing-the-Variations-of-Ensemble-Isosurfaces" href="#Visualizing-the-Variations-of-Ensemble-Isosurfaces" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/vis_variations.png" alt="Variations of Ensemble Isosurfaces"></p>
<p><strong>Goal</strong>: </p>
<ul>
<li>To visualize the variation of ensemble surfaces for a range of isovalues.</li>
</ul>
<p><strong>Implementation</strong>: </p>
<ul>
<li>Visualize the isosurface order statistics using interactive PCP</li>
<li>Color mapped surface visualization showing the distance of a member from the
median surface.  </li>
<li>Volume rendered image of the distance of each point to the median surface. </li>
</ul>
<p><a href="files/2017-3-13/vis_variations.pptx">Link to the slide</a></p>
</p><a href="#Visualizing-the-Variations-of-Ensemble-Isosurfaces"><small> 
Link to this post</small></a><hr><br><h3>Efficient Distribution-based Feature Search in Multi-field Datasets</h3><a name="Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets" href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets" class="anchor"><span class="octicon octicon-link"></span></a><small>March 13, 2017 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/2017-3-13/dist_feature1.png" alt="Distribution-Based Feature Search"></p>
<p><img src="images/2017-3-13/dist_feature2.png" alt="Distribution-Based Feature Search"></p>
<p><strong>Problem</strong>: Distribution-based (histogram-based) features has been utilized in
many volume analysis and visualization applications. However, local histogram
computation and matching is difficult in multi-field dataset due to the high
computational cost. 1. It’s infeasible to scan through the entire data space
and compute and compare local histogram in each location. 2. The number of
histogram bins increases exponentially as the number of fields or dimensions
increases, which requires a large amount of bin-by-bin comparison. 3. The high
computation cost when searching for large-sized feature, which is defined by a
large neighborhood-sized histogram.</p>
<p><strong>Proposed Solution</strong>: We utilizing bitmap indexing to reduce the search space
from the entire space domain to the voxels whose values fall into the
user-defined value range and their neighborhood voxels. Then apply the local
deposit approach to construct a local histogram in an inverse way. In the
multi-field feature search cases, we proposed two complementary algorithms for
accelerating local distribution searches. Both algorithms first approximate a
search result, and then use a low-cost refinement step to generate the final
search result. The first approach is merged-bin comparison (MBC). Instead of
comparing individual bins iteratively, we compare multiple histogram bins
between two histograms in one pass. Utilizing a property of distance measures,
our approximate search result from MBC has no false negatives so that the
refinement process only needs to remove the false positives to generate the
final result. The second approach is called sampled-active-voxels (SAV). This
utilizes stratified sampling to quickly generate approximate initial results,
which are close to the final results when compared to simple random sampling.
So the cost of refinement can thus be reduced.</p>
<p><a href="files/2017-3-13/distribution_feature.pptx">Link to the slide</a></p>
</p><a href="#Efficient-Distribution-based-Feature-Search-in-Multi-field-Datasets"><small> 
Link to this post</small></a><hr><br><h3>FY15 Progress and Updates</h3><a name="FY15-Progress-and-Updates" href="#FY15-Progress-and-Updates" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>See below for several updates on
the FY15 progress and results of the EDDA project.</p>
<p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data">Direct link to the first FY15 update</a></p>
<p><a href="pdf/fy15-osu-public.pdf">Download the OSU FY15 progress report</a></p>
</p><a href="#FY15-Progress-and-Updates"><small> 
Link to this post</small></a><hr><br><h3>A Novel Approach for Approximate Aggregations over Arrays</h3><a name="A-Novel-Approach-for-Approximate-Aggregations-over-Arrays" href="#A-Novel-Approach-for-Approximate-Aggregations-over-Arrays" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/aggregate-workflow.png" alt="approximate aggregation workflow"></p>
<p>Approximate aggregation has been a popular approach for interactive data analysis and decision making, especially on large-scale datasets. While there is clearly a need to apply this approach for scientific datasets comprising massive arrays, existing algorithms have largely been developed for relational data, and cannot handle both dimension-based and value-based predicates efficiently while maintaining accuracy. We present a novel approach for approximate aggregations over array data, using bitmap indices or bitvectors as the summary structure, as they preserve both spatial and value distribution of the data. We develop approximate aggregation algorithms using only the bitvectors and certain additional pre-aggregation statistics (equivalent to a 1-dimensional histogram) that we require. Another key development is choosing a binning strategy that can improve aggregation accuracy -- we introduce a v-optimized binning strategy and its weighted extension, and present a bitmap construction algorithm with such binning. We compare our method with other existing methods including sampling and multi-dimensional histograms, as well as the use of other binning strategies with bitmaps. We demonstrate both high accuracy and efficiency of our approach. Specifically, we show that in most cases, our method is more accurate than other methods by at least one order of magnitude. Despite achieving much higher accuracy, our method can require significantly less storage than multi-dimensional histograms.</p>
<p><a href="http://dl.acm.org/citation.cfm?doid=2791347.2791349">Link to paper</a> |
<a href="http://www.cse.ohio-state.edu/~agrawal/Slides/SSDBM2015_yw.pptx">Link to slides</a></p>
</p><a href="#A-Novel-Approach-for-Approximate-Aggregations-over-Arrays"><small> 
Link to this post</small></a><hr><br><h3>In-Situ Bitmaps Generation and Efficient Data Analysis based on Bitmaps</h3><a name="In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps" href="#In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/bitmap-mining.png" alt="bitmap correlation mining"></p>
<p>Neither the memory capacity, memory access speeds, nor disk bandwidths are increasing at the same rate as the computing power in current and upcoming parallel machines. This has led to considerable recent research on in-situ data analytics. However, many open questions remain on how to perform such analytics, especially in memory constrained systems. Building on our earlier work that demonstrated bitmap indices (bitmaps) can be a suitable summary structure for key (offline) analytics tasks,  we have developed an in-situ analysis approach that performs data reduction (such as time-steps selection) using just bitmaps, and subsequently, stores only the selected bitmaps for post-analysis. We construct compressed bitmaps on the fly, show that many kinds of in-situ analyses can be supported by bitmaps without requiring the original data (and thus reducing memory requirements for in-situ analysis), and instead of writing the original simulation output, we only write the selected bitmaps to the disks (reducing the I/O requirements). We also demonstrate that we are able to use bitmaps for key offline analysis steps. We extensively evaluate our method with different simulations and applications, and demonstrate the effectiveness of our approach.</p>
<p><a href="http://dl.acm.org/citation.cfm?doid=2749246.2749268">Link to paper</a> |
<a href="http://www.cse.ohio-state.edu/~agrawal/Slides/YuSuHPDC2015.pptx">Link to slides</a></p>
</p><a href="#In-Situ-Bitmaps-Generation-and-Efficient-Data-Analysis-based-on-Bitmaps"><small> 
Link to this post</small></a><hr><br><h3>Towards Statistically-based Error Metrics for Computationally-driven Data Triage</h3><a name="Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/size-time-tradeoff.png" alt="size vs. accuracy"></p>
<p>Computing power has increased faster than the amount of storage bandwidth or the ability to read stored data. Thus, researchers can perform large-scale, high-resolution simulations, but they are unable to load all the data into local memory. Performing queries becomes necessary; however, as the size of the data increases, queries will also be time-consuming.  Bitmap indexing was originally developed for faster query processing of read-mostly data, but has recently been used for scientific data management. It consists of a set of bit vectors, where one vector corresponds to a distinct attribute value or range of values. Each bit is mapped to a record, and the bit value is 1 if the record matches the property in focus.  Bitmap indexing is able to perform complex logical operations quickly, but for floating-point attributes, the bitmap indexing will be lossy because the bit vector values must be binned.  Different strategies have been proposed to bin bit vectors value.  </p>
<p>A review of the current literature on bitmap indexing has identified the need for a comparative error metric to assess the binning scheme of bit vectors.  This metric will incorporate multiple, conflicting criteria, such as query processing speed and index size, provide uncertainty on the assessment, and allow for a sensitivity analysis. This error metric will eventually be used as the bitmap index is being created in situ to indicate when the current binning strategy is no longer optimal.</p>
<p>Emily Casleton (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>), Jon Woodring (<em>LANL</em>)</p>
</p><a href="#Towards-Statistically-based-Error-Metrics-for-Computationally-driven-Data-Triage"><small> 
Link to this post</small></a><hr><br><h3>A Generalized Framework for Comparing across Data Representations</h3><a name="A-Generalized-Framework-for-Comparing-across-Data-Representations" href="#A-Generalized-Framework-for-Comparing-across-Data-Representations" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/scoreboard.png" alt="scoreboard"></p>
<p>In the era of big data analytics, efficient data transformation and summarization is becoming a popular approach for big data handling. Since it is almost impossible to store all the raw data, an information rich data representation can benefit data analysts immensely by reducing the size of the data to a manageable scale yet preserving the features in the data with high accuracy. In order to perform this task, we need to identify data summarization techniques which are compact, easy to compute and represent. Furthermore, a comparative framework is also need to be devised which will take different data summarization techniques and compare their effectiveness in terms of several critical parameters such as representation accuracy, storage cost, computation time, errors incurred etc. This work aims at producing such a comparative framework. Majority of the data summarization techniques try to prioritize the data by partitioning it into smaller regions and summarizing each region with appropriate representatives. Each of such representation algorithms in general can be divided into three steps: (1) data partitioning, (2) partition summarization and (3) error estimation. Therefore the accuracy and effectiveness of those representative algorithms is of prime importance to data scientists. To address these requirements, we have created a score-boarding framework which accepts several data summarization techniques and performs a global scale parameter study on them. Such a parameter study scheme is able to identify data specific transformation and summarization techniques which yields the best representation for the data. Since these transformation and summarization methods can not capture all the information the data has, appropriate error metrics are also an importance parameter in this study. By performing a comprehensive parameter study on a data set, we can analyze the results and find the best scheme of data summarization and the associated combination of parameters for it. The output of our framework is a database table where we keep track of all the test combinations and the results obtained by them in terms of data reduction, effectiveness of the summarization scheme, time taken to compute etc. By querying the output database, we can easily find the most effective algorithm and the associated parameter combinations which obtained the best result. </p>
<p>Soumya Dutta (<em>LANL</em>, <em>OSU</em>), Emily Casleton (<em>LANL</em>), Ayan Biswas (<em>LANL</em>, <em>OSU</em>), Jon Woodring (<em>LANL</em>), Jim Ahrens (<em>LANL</em>), Joanne Wendelberger (<em>LANL</em>)</p>
</p><a href="#A-Generalized-Framework-for-Comparing-across-Data-Representations"><small> 
Link to this post</small></a><hr><br><h3>Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis</h3><a name="Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/gmm-tracking.png" alt="GMM-based tracking"></p>
<p>Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7192664">Link to paper</a></p>
</p><a href="#Distribution-Driven-Extraction-and-Tracking-of-Features-for-Time-varying-Data-Analysis"><small> 
Link to this post</small></a><hr><br><h3>Efficient Local Histogram Searching via Bitmap Indexing</h3><a name="Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-searching.png" alt="local histogram search"></p>
<p>Representing features by local histograms is a proven technique in several volume analysis and visualization applications including feature tracking and transfer function design. The efficiency of these applications, however, is hampered by the high computational complexity of local histogram computation and matching. In this paper, we propose a novel algorithm to accelerate local histogram search by leveraging bitmap indexing. Our method avoids exhaustive searching of all voxels in the spatial domain by examining only the voxels whose values fall within the value range of user-defined local features and their neighborhood. Based on the idea that the value range of local features is in general much smaller than the dynamic range of the entire dataset, we propose a local voting scheme to construct the local histograms so that only a small number of voxels need to be examined. Experimental results show that our method can reduce much computational workload compared to the conventional approaches. To demonstrate the utility of our method, an interactive interface was developed to assist users in defining target features as local histograms and identify the locations of these features in the dataset.</p>
<p><a href="http://onlinelibrary.wiley.com/doi/10.1111/cgf.12620/abstract">Link to paper</a></p>
</p><a href="#Efficient-Local-Histogram-Searching-via-Bitmap-Indexing"><small> 
Link to this post</small></a><hr><br><h3>A Compact Multivariate Histogram Representation for Query-driven Visualization</h3><a name="A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/histogram-query.png" alt="pipeline for histogram query"></p>
<p>As the size of data continues to increase, distribution-based methods become increasingly more important for data summarization and queries. To represent the distribution from a dataset without relying on a particular parametric model, histograms are widely used in many applications as it is simple to create and efficient to query. For multivariate scientific datasets, however, storing multivariate histograms in the form of multi-dimensional arrays is very expensive as the size of the histogram grows exponentially with the number of variables. In this paper, we present a compact structure to store multivariate histograms to reduce its huge space cost while supporting different kinds of histogram queries efficiently. A data space transformation is employed first to transform the large multi-dimensional array to a much smaller array. Dictionaries are constructed to encode this transformation. Then, the multivariate histogram is represented as a sequence of index and frequency pairs where the indices are represented as bitstrings computed from a space filling curve traversal of the transformed array. With this compact representation, the storage cost for the histograms is reduced. Based on our representation, we also present several common types of queries such as histogram marginalization, bin-merging and computation of conditional probability. We parallelize both the histogram computation and queries to improve its efficiency. We present several query-driven visualization applications to explore and analyze multivariate scientific datasets. Experimental results to study the performance of our framework in terms of scalability and space cost are also discussed.</p>
<p>Kewei Lu, Han-Wei Shen. <em>The 5th IEEE Symposium on Large Data Analysis and Visualization</em>, 2015.</p>
</p><a href="#A-Compact-Multivariate-Histogram-Representation-for-Query-driven-Visualization"><small> 
Link to this post</small></a><hr><br><h3>An Uncertainty-Driven Approach to Vortex Analysis Using Oracle Consensus and Spatial Proximity</h3><a name="An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/vortex-analysis.png" alt="vortex detection"></p>
<p>Although vortex analysis and detection have been extensively investigated in the past, none of the existing techniques are able to provide fully robust and reliable identification results. Local vortex detection methods are popular as they are efficient and easy to implement, and produce binary outputs based on a user-specified, hard threshold. However, vortices are global features, which present challenges for local detectors. On the other hand, global detectors are computationally intensive and require considerable user input. In this work, we propose a consensus-based uncertainty model and introduce spatial proximity to enhance vortex detection results obtained using point-based methods. We use four existing local vortex detectors and convert their outputs into fuzzy possibility values using a sigmoid-based soft-thresholding approach. We apply a majority voting scheme that enables us to identify candidate vortex regions with a higher degree of confidence. Then, we introduce spatial proximity- based analysis to discern the final vortical regions. Thus, by using spatial proximity coupled with fuzzy inputs, we propose a novel uncertainty analysis approach for vortex detection. We use expert’s input to better estimate the system parameters and results from two real-world data sets demonstrate the efficacy of our method.</p>
<p><a href="http://web.cse.ohio-state.edu/~biswas/template_2.pdf">Link to paper</a></p>
</p><a href="#An-Uncertainty-Driven-Approach-to-Vortex-Analysis-Using-Oracle-Consensus-and-Spatial-Proximity"><small> 
Link to this post</small></a><hr><br><h3>Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation</h3><a name="Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/rotating-stall.png" alt="detection of stall"></p>
<p>Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data inmore detail. To achieve this, we use a statistical anomaly detection method to locate possible stall inception.  We further derive algorithms from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.</p>
<p><a href="http://dx.doi.org/10.1109/TVCG.2015.2467952">Link to paper</a></p>
</p><a href="#Visualization-and-Analysis-of-Rotating-Stall-for-Transonic-Jet-Engine-Simulation"><small> 
Link to this post</small></a><hr><br><h3>Software for Distribution Data Modeling and Visualization</h3><a name="Software-for-Distribution-Data-Modeling-and-Visualization" href="#Software-for-Distribution-Data-Modeling-and-Visualization" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The <a href="https://sites.google.com/site/gravityvisdb/edda">EDDA library</a> aims at visualizing distribution data for uncertainty analysis.  </p>
<p>The goal is to provide a unified data model with generic distribution representations for the development of uncertainty visualization algorithms.  The distribution models to support will be parametric distributions like Gaussian and GMM, un-parametric distributions like histogram and KDE, as well as joint distributions.  These are encapsulated into C++ template classes.  Coupled with our experiences on developing regular and curvilinear-grid datasets in OSUFlow, we provide an API allowing to query for the distribution of a given 3D position.  The return of the query can be either the interpolated distribution or a Monte-Carlo sample of the distribution, depending on the need of the visualization algorithm.  We also provide distribution arithmetic and analysis tools including project ITL.</p>
<p>Chun-Ming Chen, <em>et al.</em> (<em>OSU</em>)</p>
</p><a href="#Software-for-Distribution-Data-Modeling-and-Visualization"><small> 
Link to this post</small></a><hr><br><h3>Uncertainty Modeling and Error Reduction for Pathline Computation in Time-varying Flow Fields</h3><a name="Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/uncertainty-modeling-1.png" alt="uncertain pathline"> <img src="images/uncertainty-modeling-2.png" alt="uncertain-pathline"></p>
<p>When the spatial and temporal resolutions of a time-varying simulation become very high, it is not possible to process or store data from every time step due to the high computation and storage cost. Although using uniformly down-sampled data for visualization is a common practice, important information in the un-stored data can be lost. Currently, linear interpolation is a popular method used to approximate data between the stored time steps. For pathline computation, however, errors from the interpolated velocity in the time dimension can accumulate quickly and make the trajectories rather unreliable. To inform the scientist the error involved in the visualization, it is important to quantify and display the uncertainty, and more importantly, to reduce the error whenever possible. In this paper, we present an algorithm to model temporal interpolation error, and an error reduction scheme to improve the data accuracy for temporally down-sampled data. We show that it is possible to compute polynomial regression and measure the interpolation errors incrementally with one sequential scan of the time-varying flow field. We also show empirically that when the data sequence is fitted with least-squares regression, the errors can be approximated with a Gaussian distribution. With the end positions of particle traces stored, we show that our error modeling scheme can better estimate the intermediate particle trajectories between the stored time steps based on a maximum likelihood method that utilizes forward and backward particle traces.</p>
<p><a href="http://dx.doi.org/10.1109/PACIFICVIS.2015.715638">Link to paper</a></p>
</p><a href="#Uncertainty-Modeling-and-Error-Reduction-for-Pathline-Computation-in-Time-varying-Flow-Fields"><small> 
Link to this post</small></a><hr><br><h3>Association Analysis for Visual Exploration of Multivariate Scientific Data Sets</h3><a name="Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/association-analysis.png" alt="association analysis"></p>
<p>The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data’s multi-faceted properties. We present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.</p>
<p><a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=7192697">Link to paper</a></p>
</p><a href="#Association-Analysis-for-Visual-Exploration-of-Multivariate-Scientific-Data-Sets"><small> 
Link to this post</small></a><hr><br><h3>Self-Adaptive Density Estimation of Particle Data</h3><a name="Self-Adaptive-Density-Estimation-of-Particle-Data" href="#Self-Adaptive-Density-Estimation-of-Particle-Data" class="anchor"><span class="octicon octicon-link"></span></a><small>October 25, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p><img src="images/density-estimate.png" alt="cosmology particles and density field"></p>
<p>We conducted a study of density estimation, the conversion of discrete particle positions to a continuous field of particle density defined over a 3D Cartesian grid. The study features a methodology for evaluating the accuracy and performance of various density estimation methods, results of that evaluation for four density estimators, and a large-scale parallel algorithm for a self-adaptive method that computes a Voronoi tessellation as an intermediate step. We demonstrated the performance and scalability of our parallel algorithm on a supercomputer when estimating the density of 100 million particles over 500 billion grid points.</p>
<p>Peterka, <em>et al.</em> (<em>ANL</em>) Submitted to <em>SIAM Journal on Scientific Computing SISC Special Section on CSE15: Software and Big Data</em>, 2015.</p>
</p><a href="#Self-Adaptive-Density-Estimation-of-Particle-Data"><small> 
Link to this post</small></a><hr><br><h3>SDMAV Kick-Off PI Meeting</h3><a name="SDMAV-Kick-Off-PI-Meeting" href="#SDMAV-Kick-Off-PI-Meeting" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>On Tuesday, January 13-15, there will be a Principal Investigator (PI)
Meeting, for projects funded under the Scientific Data Management,
Analysis and Visualization (SDMAV) by DOE SC ASCR, in Walnut Creek, CA.
EDDA PIs and investigators will be present for this meeting. Below
are links to download materials to be presented at this meeting for the
EDDA project.</p>
<ul>
<li><a href="files/2015-1-6/EDDA-Jan2015-Handout.docx">Handout</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Quad.ppt">Quad Chart</a></li>
<li><a href="files/2015-1-6/EDDA-Jan2015-Poster.pdf">Poster</a></li>
</ul>
</p><a href="#SDMAV-Kick-Off-PI-Meeting"><small> 
Link to this post</small></a><hr><br><h3>Logo Explanation</h3><a name="Logo-Explanation" href="#Logo-Explanation" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>The acronym for our project is EDDA, which if you google for it you find
the description for Edda on <a href="http://en.wikipedia.org/wiki/Edda">wikipedia</a>.</p>
<blockquote>
<p>The term &quot;Edda&quot; (/ˈɛdə/; Old Norse Edda, plural Eddur) applies to 
the Old Norse Poetic Edda and Prose Edda, both of which were written 
down in Iceland during the 13th century in Icelandic, although they 
contain material from earlier traditional sources, reaching into the 
Viking Age. The books are the main sources of medieval skaldic tradition 
in Iceland and Norse mythology. </p>
</blockquote>
<p>Runic alphabets, in particular <em>futhark</em> (&quot;th&quot; is the thorn: þ, fuþark), 
was used by Scandanavian (Norse) and its use was noted use in Eddic lore.</p>
<blockquote>
<p>In Norse mythology, the runic alphabet is attested to a divine origin 
(Old Norse: reginkunnr). This is attested as early as on the Noleby 
Runestone from approximately 600 AD that reads Runo fahi raginakundo 
toj[e&#39;k]a..., meaning &quot;I prepare the suitable divine rune...&quot; and in 
an attestation from the 9th century on the Sparlösa Runestone, which 
reads Ok rað runaR þaR rægi[n]kundu, meaning &quot;And interpret the runes 
of divine origin&quot;.</p>
</blockquote>
<p>J.R.R. Tolkien used derivatives of <em>futhark</em> to describe the alphabet
used by the dwarves and even created his own called <em>Cirth</em>.</p>
<p>Well, what does this all mean for our logo? It&#39;s the transliterated
version of EDDA into <em>elder futhark</em>.</p>
<ul>
<li>E = ehwaz = &quot;M&quot; like character</li>
<li>D = dagaz = &quot;infinity&quot; like character</li>
<li>A = ansuz = &quot;F&quot; like character</li>
</ul>
</p><a href="#Logo-Explanation"><small> 
Link to this post</small></a><hr><br><h3>Introduction</h3><a name="Introduction" href="#Introduction" class="anchor"><span class="octicon octicon-link"></span></a><small>January 6, 2015 -- <a href="https://github.com/jonwoodring">Jon Woodring</a><br></small><p><p>As it becomes more difficult to analyze large-scale simulation output at full
resolution, users will have to review and identify regions of interest by
transforming data into compact information descriptors that characterize
simulation results and allow detailed analysis on demand. This is because
exascale architectures will be much more constrained with respect to data
movement, and <em>in situ</em> data processing will be the norm, where the goals are to
fit the total amount of output data within a budget, to summarize and triage
data based on content, and to classify and index data to facilitate efficient
offline analysis. In addition, <em>in situ</em> analysis must be performed in a time 
and space efficient fashion, not only to avoid slowing down the simulation, but
also to not consume too much memory.  </p>
<p>Among many different feature descriptors,
the statistical information derived from data samples is a promising approach
to taming the big data avalanche, because data distributions computed from a
population can compactly describe the presence and characteristics of salient
features with minimal data movement.  The ability to computationally summarize
and process data using distributions provides an efficient and representative
capture of the information content of a large-scale data set. This
representation can adjust to size and resource constraints, with the added
benefit that uncertainty can be quantified and communicated.  </p>
<p>In this project,
<em>we posit that with the growing number of cores per node, with increasing memory
and I/O constraints in emerging extreme-scale platforms, it will be feasible
and desirable to compute distributions at simulation time, perform
memory-efficient</em> in situ <em>analysis using distributions, and save distributions
as a space-efficient summarization for on-demand, offline visualization and
analysis of salient features.</em> The key development will be a novel
distribution-based analysis and visualization framework based on in situ pro-
cessing of extreme-scale scientific data. Our goals are to ensure that
scientists can easily obtain an overview of the entire data set regardless of
the size of the simulation; understand the characteristics and locations of the
features; easily interact with the data and select regions and features of
interest; and perform all the analysis tasks with a small memory footprint.</p>
</p><a href="#Introduction"><small> 
Link to this post</small></a><hr><br></p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small><script src="javascripts/scale.fix.js"></script></body></html>